# Personalized Daily Arxiv Papers 11/28/2023
Total relevant papers: 123

Table of contents with paper titles:

0. [Evaluating the Robustness to Instructions of Large Language Models](https://arxiv.org/abs/2308.14306)
**Authors:** Yuansheng Ni, Sichao Jiang, Xinyu wu, Hui Shen, Yuli Zhou

1. [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/abs/2311.07590)
**Authors:** Jérémy Scheurer, Mikita Balesni, Marius Hobbhahn

2. [A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective](https://arxiv.org/abs/2311.16065)
**Authors:** Xianghua Xie, Chen Hu, Hanchi Ren, Jingjing Deng

3. [AdaptGuard: Defending Against Universal Attacks for Model Adaptation](https://arxiv.org/abs/2303.10594)
**Authors:** Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan

4. [ViT-Lens-2: Gateway to Omni-modal Intelligence](https://arxiv.org/abs/2311.16081)
**Authors:** Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou

5. [Utilizing Explainability Techniques for Reinforcement Learning Model Assurance](https://arxiv.org/abs/2311.15838)
**Authors:** Alexander Tapley, Kyle Gatesman, Luis Robaina, Brett Bissey, Joseph Weissman

6. [Forecasting Auxiliary Energy Consumption for Electric Heavy-Duty Vehicles](https://arxiv.org/abs/2311.16003)
**Authors:** Yuantao Fan, Zhenkan Wang, Sepideh Pashami, Slawomir Nowaczyk, Henrik Ydreskog

7. [Have we built machines that think like people?](https://arxiv.org/abs/2311.16093)
**Authors:** Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, Eric Schulz

8. [Diagnosis driven Anomaly Detection for CPS](https://arxiv.org/abs/2311.15924)
**Authors:** Henrik S. Steude, Lukas Moddemann, Alexander Diedrich, Jonas Ehrhardt, Oliver Niggemann

9. [HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](https://arxiv.org/abs/2311.12454)
**Authors:** Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee

10. [Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence](https://arxiv.org/abs/2311.15782)
**Authors:** Svetlana Pavlitska, Hannes Grolig, J. Marius Zöllner

11. [GloNets: Globally Connected Neural Networks](https://arxiv.org/abs/2311.15947)
**Authors:** Antonio Di Cecco, Carlo Metta, Marco Fantozzi, Francesco Morandin, Maurizio Parton

12. [Temporal Action Localization for Inertial-based Human Activity Recognition](https://arxiv.org/abs/2311.15831)
**Authors:** Marius Bock, Michael Moeller, Kristof Van Laerhoven

13. [Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes](https://arxiv.org/abs/2311.15946)
**Authors:** Tuan-Dung Le, Zhuqi Miao, Samuel Alvarado, Brittany Smith, William Paiva, Thanh Thieu

14. [TorchRL: A data-driven decision-making library for PyTorch](https://arxiv.org/abs/2306.00577)
**Authors:** Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, Vincent Moens

15. [Understanding plasticity in neural networks](https://arxiv.org/abs/2303.01486)
**Authors:** Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, Will Dabney

16. [Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models](https://arxiv.org/abs/2311.16017)
**Authors:** Stephen MacNeil, Paul Denny, Andrew Tran, Juho Leinonen, Seth Bernstein, Arto Hellas, Sami Sarsa, Joanne Kim

17. [Energy Discrepancies: A Score-Independent Loss for Energy-Based Models](https://arxiv.org/abs/2307.06431)
**Authors:** Tobias Schröder, Zijing Ou, Jen Ning Lim, Yingzhen Li, Sebastian J. Vollmer, Andrew B. Duncan

18. [Metric Space Magnitude for Evaluating Unsupervised Representation Learning](https://arxiv.org/abs/2311.16054)
**Authors:** Katharina Limbeck, Rayna Andreeva, Rik Sarkar, Bastian Rieck

19. [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)
**Authors:** Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie

20. [PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators](https://arxiv.org/abs/2310.19106)
**Authors:** Antonin Sulc, Raimund Kammering, Annika Eichler, Tim Wilksen

21. [DeepTSF: Codeless machine learning operations for time series forecasting](https://arxiv.org/abs/2308.00709)
**Authors:** Sotiris Pelekis, Evangelos Karakolis, Theodosios Pountridis, George Kormpakis, George Lampropoulos, Spiros Mouzakitis, Dimitris Askounis

22. [Data Generation for Post-OCR correction of Cyrillic handwriting](https://arxiv.org/abs/2311.15896)
**Authors:** Evgenii Davydkin, Aleksandr Markelov, Egor Iuldashev, Anton Dudkin, Ivan Krivorotov

23. [Nova$^+$: Generative Language Models for Binaries](https://arxiv.org/abs/2311.13721)
**Authors:** Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang

24. [Assessing Deep Neural Networks as Probability Estimators](https://arxiv.org/abs/2111.08239)
**Authors:** Yu Pan, Kwo-Sen Kuo, Michael L. Rilee, Hongfeng Yu

25. [Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation](https://arxiv.org/abs/2311.16091)
**Authors:** Jiachen Li, David Isele, Kanghoon Lee, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer

26. [From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding](https://arxiv.org/abs/2304.00553)
**Authors:** Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, Cewu Lu

27. [RIDE: Real-time Intrusion Detection via Explainable Machine Learning Implemented in a Memristor Hardware Architecture](https://arxiv.org/abs/2311.16018)
**Authors:** Jingdi Chen, Lei Zhang, Joseph Riem, Gina Adam, Nathaniel D. Bastian, Tian Lan

28. [A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems](https://arxiv.org/abs/2311.04014)
**Authors:** Cheng Yin, Yi Chen

29. [Exploring Artificial Intelligence Methods for Energy Prediction in Healthcare Facilities: An In-Depth Extended Systematic Review](https://arxiv.org/abs/2311.15807)
**Authors:** Marjan FatehiJananloo, Helen Stopps, J. J. McArthur

30. [A precise symbolic emulator of the linear matter power spectrum](https://arxiv.org/abs/2311.15865)
**Authors:** Deaglan J. Bartlett, Lukas Kammerer, Gabriel Kronberger, Harry Desmond, Pedro G. Ferreira, Benjamin D. Wandelt, Bogdan Burlacu, David Alonso, Matteo Zennaro

31. [BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification](https://arxiv.org/abs/2311.16083)
**Authors:** Dmitri Roussinov, Serge Sharoff

32. [RCT Rejection Sampling for Causal Estimation Evaluation](https://arxiv.org/abs/2307.15176)
**Authors:** Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya

33. [MAST: Model-Agnostic Sparsified Training](https://arxiv.org/abs/2311.16086)
**Authors:** Yury Demidovich, Grigory Malinovsky, Egor Shulgin, Peter Richtárik

34. [Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs](https://arxiv.org/abs/2311.15781)
**Authors:** Simone Conia, Min Li, Daniel Lee, Umar Farooq Minhas, Ihab Ilyas, Yunyao Li

35. [Sentiment analysis with adaptive multi-head attention in Transformer](https://arxiv.org/abs/2310.14505)
**Authors:** Fanfei Meng, David Demeter

36. [Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning](https://arxiv.org/abs/2102.12920)
**Authors:** Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid

37. [Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers](https://arxiv.org/abs/2311.15983)
**Authors:** Yilun Liu, Difan Jiao, Ashton Anderson

38. [Rethinking Privacy in Machine Learning Pipelines from an Information Flow Control Perspective](https://arxiv.org/abs/2311.15792)
**Authors:** Lukas Wutschitz, Boris Köpf, Andrew Paverd, Saravan Rajmohan, Ahmed Salem, Shruti Tople, Santiago Zanella-Béguelin, Menglin Xia, Victor Rühle

39. [What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models](https://arxiv.org/abs/2310.06627)
**Authors:** Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao

40. [A systematic study comparing hyperparameter optimization engines on tabular data](https://arxiv.org/abs/2311.15854)
**Authors:** Balazs Kegl

41. [Attend Who is Weak: Enhancing Graph Condensation via Cross-Free Adversarial Training](https://arxiv.org/abs/2311.15772)
**Authors:** Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, Di Wu

42. [Self-Guided Diffusion Models](https://arxiv.org/abs/2210.06462)
**Authors:** Vincent Tao Hu, David W Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek

43. [Long-Range Neural Atom Learning for Molecular Graphs](https://arxiv.org/abs/2311.01276)
**Authors:** Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han

44. [The Chosen One: Consistent Characters in Text-to-Image Diffusion Models](https://arxiv.org/abs/2311.10093)
**Authors:** Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski

45. [Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges](https://arxiv.org/abs/2311.15766)
**Authors:** Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang

46. [Auto-CsiNet: Scenario-customized Automatic Neural Network Architecture Generation for Massive MIMO CSI Feedback](https://arxiv.org/abs/2311.15950)
**Authors:** Xiangyi Li, Jiajia Guo, Chao-Kai Wen, Shi Jin

47. [Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs](https://arxiv.org/abs/2311.15759)
**Authors:** Yunxin Li, Baotian Hu, Wei Wang, Xiaochun Cao, Min Zhang

48. [Learning Multi-Frequency Partial Correlation Graphs](https://arxiv.org/abs/2311.15756)
**Authors:** Gabriele D'Acunto, Paolo Di Lorenzo, Francesco Bonchi, Stefania Sardellitti, Sergio Barbarossa

49. [Scheduling and Communication Schemes for Decentralized Federated Learning](https://arxiv.org/abs/2311.16021)
**Authors:** Bahaa-Eldin Ali Abdelghany, Ana Fernández-Vilas, Manuel Fernández-Veiga, Nashwa El-Bendary, Ammar M. Hassan, Walid M. Abdelmoez

50. [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)
**Authors:** Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, Jiwen Lu

51. [An HCAI Methodological Framework: Putting It Into Action to Enable Human-Centered AI](https://arxiv.org/abs/2311.16027)
**Authors:** Wei Xu, Zaifeng Gao, Marvin Dainoff

52. [Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces](https://arxiv.org/abs/2211.14400)
**Authors:** Jonathan W. Siegel

53. [AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval](https://arxiv.org/abs/2311.14084)
**Authors:** Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, Xueqi Cheng

54. [Towards Transfer Learning for Large-Scale Image Classification Using Annealing-based Quantum Boltzmann Machines](https://arxiv.org/abs/2311.15966)
**Authors:** Daniëlle Schuman, Leo Sünkel, Philipp Altmann, Jonas Stein, Christoph Roch, Thomas Gabor, Claudia Linnhoff-Popien

55. [From Pixels to Titles: Video Game Identification by Screenshots using Convolutional Neural Networks](https://arxiv.org/abs/2311.15963)
**Authors:** Fabricio Breve

56. [A Social-aware Gaussian Pre-trained Model for Effective Cold-start Recommendation](https://arxiv.org/abs/2311.15790)
**Authors:** Siwei Liu, Xi Wang, Craig Macdonald, Iadh Ounis

57. [Transformer-QEC: Quantum Error Correction Code Decoding with Transferable Transformers](https://arxiv.org/abs/2311.16082)
**Authors:** Hanrui Wang, Pengyu Liu, Kevin Shao, Dantong Li, Jiaqi Gu, David Z. Pan, Yongshan Ding, Song Han

58. [FLASC: A Flare-Sensitive Clustering Algorithm: Extending HDBSCAN* for Detecting Branches in Clusters](https://arxiv.org/abs/2311.15887)
**Authors:** D. M. Bot, J. Peeters, J. Liesenborgs, J. Aerts

59. [Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation](https://arxiv.org/abs/2311.14353)
**Authors:** Yasumasa Kano, Katsuhito Sudoh, Satoshi Nakamura

60. [Nodal Hydraulic Head Estimation through Unscented Kalman Filter for Data-driven Leak Localization in Water Networks](https://arxiv.org/abs/2311.15875)
**Authors:** Luis Romero-Ben, Paul Irofti, Florin Stoican, Vicenç Puig

61. [Online Estimation and Optimization of Utility-Based Shortfall Risk](https://arxiv.org/abs/2111.08805)
**Authors:** Vishwajit Hegde, Arvind S. Menon, L. A. Prashanth, Krishna Jagannathan

62. [A new fuzzy multi-attribute group decision-making method based on TOPSIS and optimization models](https://arxiv.org/abs/2311.15933)
**Authors:** Qixiao Hu, Shiquan Zhang, Chaolang Hu, Yuetong Liu

63. [Improved Data Generation for Enhanced Asset Allocation: A Synthetic Dataset Approach for the Fixed Income Universe](https://arxiv.org/abs/2311.16004)
**Authors:** Szymon Kubiak, Tillman Weyde, Oleksandr Galkin, Dan Philps, Ram Gopal

64. [RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization](https://arxiv.org/abs/2311.15876)
**Authors:** Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Jin Sung Kim, Yong Bae Kim, Jong Chul Ye

65. [YUAN 2.0: A Large Language Model with Localized Filtering-based Attention](https://arxiv.org/abs/2311.15786)
**Authors:** Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang

66. [Should We Learn Most Likely Functions or Parameters?](https://arxiv.org/abs/2311.15990)
**Authors:** Shikai Qiu, Tim G. J. Rudner, Sanyam Kapoor, Andrew Gordon Wilson

67. [A deep reinforcement learning model for predictive maintenance planning of road assets: Integrating LCA and LCCA](https://arxiv.org/abs/2112.12589)
**Authors:** Moein Latifi, Fateme Golivand Darvishvand, Omid Khandel, Mobin Latifi Nowsoud

68. [Soil Organic Carbon Estimation from Climate-related Features with Graph Neural Network](https://arxiv.org/abs/2311.15979)
**Authors:** Weiying Zhao, Natalia Efremova

69. [Towards Adaptive RF Fingerprint-based Authentication of IIoT devices](https://arxiv.org/abs/2311.15888)
**Authors:** Emmanuel Lomba, Ricardo Severino, Ana Fernández Vilas

70. [Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines](https://arxiv.org/abs/2311.15960)
**Authors:** Yu-An Lin, Chen-Tao Lee, Guan-Ting Liu, Pu-Jen Cheng, Shao-Hua Sun

71. [Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework](https://arxiv.org/abs/2311.15993)
**Authors:** Shaobo Wang, Xiangdong Zhang, Junchi Yan

72. [RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment](https://arxiv.org/abs/2305.19599)
**Authors:** Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Shengcai Liao, Xiaodan Liang

73. [Machine learning-based decentralized TDMA for VLC IoT networks](https://arxiv.org/abs/2311.14078)
**Authors:** Armin Makvandi, Yousef Seifi Kavian

74. [Tell2Design: A Dataset for Language-Guided Floor Plan Generation](https://arxiv.org/abs/2311.15941)
**Authors:** Sicong Leng, Yang Zhou, Mohammed Haroon Dupty, Wee Sun Lee, Sam Conrad Joyce, Wei Lu

75. [XLB: Distributed Multi-GPU Lattice Boltzmann Simulation Framework for Differentiable Scientific Machine Learning](https://arxiv.org/abs/2311.16080)
**Authors:** Mohammadmehdi Ataei, Hesam Salehipour

76. [DUnE: Dataset for Unified Editing](https://arxiv.org/abs/2311.16087)
**Authors:** Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, Derry Wijaya

77. [Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation](https://arxiv.org/abs/2203.05400)
**Authors:** Toni Karvonen

78. [Generative AI and US Intellectual Property Law](https://arxiv.org/abs/2311.16023)
**Authors:** Cherie M Poland

79. [On Bringing Robots Home](https://arxiv.org/abs/2311.16098)
**Authors:** Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, Lerrel Pinto

80. [CheapNET: Improving Light-weight speech enhancement network by projected loss function](https://arxiv.org/abs/2311.15959)
**Authors:** Kaijun Tan, Benzhe Dai, Jiakui Li, Wenyu Mao

81. [FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax](https://arxiv.org/abs/2311.15813)
**Authors:** Yu Lu, Linchao Zhu, Hehe Fan, Yi Yang

82. [A Neural Framework for Generalized Causal Sensitivity Analysis](https://arxiv.org/abs/2311.16026)
**Authors:** Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar

83. [Sensitivity-Based Layer Insertion for Residual and Feedforward Neural Networks](https://arxiv.org/abs/2311.15995)
**Authors:** Evelyn Herberg, Roland Herzog, Frederik Köhne, Leonie Kreis, Anton Schiela

84. [Multi-Agent Reinforcement Learning for Power Control in Wireless Networks via Adaptive Graphs](https://arxiv.org/abs/2311.15858)
**Authors:** Lorenzo Mario Amorosa, Marco Skocaj, Roberto Verdone, Deniz Gündüz

85. [Closing the ODE-SDE gap in score-based diffusion models through the Fokker-Planck equation](https://arxiv.org/abs/2311.15996)
**Authors:** Teo Deveney, Jan Stanczuk, Lisa Maria Kreusser, Chris Budd, Carola-Bibiane Schönlieb

86. [MetaDefa: Meta-learning based on Domain Enhancement and Feature Alignment for Single Domain Generalization](https://arxiv.org/abs/2311.15906)
**Authors:** Can Sun, Hao Zheng, Zhigang Hu, Liu Yang, Meiguang Zheng, Bo Xu

87. [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing](https://arxiv.org/abs/2309.16883)
**Authors:** Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen

88. [Stability-Informed Initialization of Neural Ordinary Differential Equations](https://arxiv.org/abs/2311.15890)
**Authors:** Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk

89. [Towards Responsible Governance of Biological Design Tools](https://arxiv.org/abs/2311.15936)
**Authors:** Richard Moulange, Max Langenkamp, Tessa Alexanian, Samuel Curtis, Morgan Livingston

90. [RobustState: Boosting Fidelity of Quantum State Preparation via Noise-Aware Variational Training](https://arxiv.org/abs/2311.16035)
**Authors:** Hanrui Wang, Yilian Liu, Pengyu Liu, Jiaqi Gu, Zirui Li, Zhiding Liang, Jinglei Cheng, Yongshan Ding, Xuehai Qian, Yiyu Shi, David Z. Pan, Frederic T. Chong, Song Han

91. [Dimensionality Reduction and Wasserstein Stability for Kernel Regression](https://arxiv.org/abs/2203.09347)
**Authors:** Stephan Eckstein, Armin Iske, Mathias Trabs

92. [Efficient Perception, Planning, and Control Algorithms for Vision-Based Automated Vehicles](https://arxiv.org/abs/2209.07042)
**Authors:** Der-Hau Lee

93. [Neuradicon: operational representation learning of neuroimaging reports](https://arxiv.org/abs/2107.10021)
**Authors:** Henry Watkins, Robert Gray, Adam Julius, Yee-Haur Mah, Walter H. L. Pinaya, Paul Wright, Ashwani Jha, Holger Engleitner, Jorge Cardoso, Sebastien Ourselin, Geraint Rees, Rolf Jaeger, Parashkev Nachev

94. [MEDITRON-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079)
**Authors:** Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut

95. [Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks](https://arxiv.org/abs/2311.11913)
**Authors:** Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum

96. [Using Decentralized Aggregation for Federated Learning with Differential Privacy](https://arxiv.org/abs/2311.16008)
**Authors:** Hadeel Abd El-Kareem, Abd El-Moaty Saleh, Ana Fernández-Vilas, Manuel Fernández-Veiga, asser El-Sonbaty

97. [Over-Squashing in Riemannian Graph Neural Networks](https://arxiv.org/abs/2311.15945)
**Authors:** Julia Balla

98. [Machine learning and Topological data analysis identify unique features of human papillae in 3D scans](https://arxiv.org/abs/2307.06255)
**Authors:** Rayna Andreeva, Anwesha Sarkar, Rik Sarkar

99. [Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift](https://arxiv.org/abs/2311.15961)
**Authors:** Jiawei Ge, Shange Tang, Jianqing Fan, Cong Ma, Chi Jin

100. [Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale](https://arxiv.org/abs/2311.15816)
**Authors:** Soyed Tuhin Ahmed, Kamal Danouchi, Michael Hefenbrock, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori

101. [Low-degree learning and the metric entropy of polynomials](https://arxiv.org/abs/2203.09659)
**Authors:** Alexandros Eskenazis, Paata Ivanisvili, Lauritz Streck

102. [WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models](https://arxiv.org/abs/2311.15930)
**Authors:** Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, Pascal Vincent

103. [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)
**Authors:** Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez

104. [Reinforcement Learning for Wildfire Mitigation in Simulated Disaster Environments](https://arxiv.org/abs/2311.15925)
**Authors:** Alexander Tapley, Marissa Dotter, Michael Doyle, Aidan Fennelly, Dhanuj Gandikota, Savanna Smith, Michael Threet, Tim Welsh

105. [On the Effectiveness of Log Representation for Log-based Anomaly Detection](https://arxiv.org/abs/2308.08736)
**Authors:** Xingfang Wu, Heng Li, Foutse Khomh

106. [BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights](https://arxiv.org/abs/2311.16075)
**Authors:** François Remy, Kris Demuynck, Thomas Demeester

107. [FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations](https://arxiv.org/abs/2211.14309)
**Authors:** Christian Diller, Thomas Funkhouser, Angela Dai

108. [Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models](https://arxiv.org/abs/2311.16103)
**Authors:** Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan

109. [Automated Measurement of Vascular Calcification in Femoral Endarterectomy Patients Using Deep Learning](https://arxiv.org/abs/2311.16001)
**Authors:** Alireza Bagheri Rajeoni, Breanna Pederson, Daniel G. Clair, Susan M. Lessner, Homayoun Valafar

110. [Car-Following Models: A Multidisciplinary Review](https://arxiv.org/abs/2304.07143)
**Authors:** Tianya Zhang, Peter J. Jin, Alexandre Bayen, Ph. D., Benedetto Piccoli

111. [ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting](https://arxiv.org/abs/2310.13258)
**Authors:** Kushal Kedia, Prithwish Dan, Atiksh Bhardwaj, Sanjiban Choudhury

112. [Machine Learning-Enhanced Aircraft Landing Scheduling under Uncertainties](https://arxiv.org/abs/2311.16030)
**Authors:** Yutian Pang, Peng Zhao, Jueming Hu, Yongming Liu

113. [Cell Maps Representation For Lung Adenocarcinoma Growth Patterns Classification In Whole Slide Images](https://arxiv.org/abs/2311.15847)
**Authors:** Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, Nasir Rajpoot, Shan E Ahmed Raza

114. [A Quantitative Approach to Understand Self-Supervised Models as Cross-lingual Feature Extractors](https://arxiv.org/abs/2311.15954)
**Authors:** Shuyue Stella Li, Beining Xu, Xiangyu Zhang, Hexin Liu, Wenhan Chao, Leibny Paola Garcia

115. [Test-time Adaptation of Discriminative Models via Diffusion Generative Feedback](https://arxiv.org/abs/2311.16102)
**Authors:** Mihir Prabhudesai, Tsung-Wei Ke, Alexander C. Li, Deepak Pathak, Katerina Fragkiadaki

116. [CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](https://arxiv.org/abs/2306.00349)
**Authors:** Jiachen Sun, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Z. Morley Mao, Chaowei Xiao

117. [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks](https://arxiv.org/abs/2305.13547)
**Authors:** Haoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Dongsheng Li, Dacheng Tao

118. [AST: Effective Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories](https://arxiv.org/abs/2310.10541)
**Authors:** Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam

119. [Efficient Pre-training for Localized Instruction Generation of Videos](https://arxiv.org/abs/2311.15964)
**Authors:** Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller

120. [A Fully Data-Driven Approach for Realistic Traffic Signal Control Using Offline Reinforcement Learning](https://arxiv.org/abs/2311.15920)
**Authors:** Jianxiong Li, Shichao Lin, Tianyu Shi, Chujie Tian, Yu Mei, Jian Song, Xianyuan Zhan, Ruimin Li

121. [Replay across Experiments: A Natural Extension of Off-Policy RL](https://arxiv.org/abs/2311.15951)
**Authors:** Dhruva Tirumala, Thomas Lampe, Jose Enrique Chen, Tuomas Haarnoja, Sandy Huang, Guy Lever, Ben Moran, Tim Hertweck, Leonard Hasenclever, Martin Riedmiller, Nicolas Heess, Markus Wulfmeier

122. [Physics-informed neural networks for transformed geometries and manifolds](https://arxiv.org/abs/2311.15940)
**Authors:** Samuel Burbulla

---
## 0. [Evaluating the Robustness to Instructions of Large Language Models](https://arxiv.org/abs/2308.14306) <a name="link0"></a>
**ArXiv ID:** 2308.14306
**Authors:** Yuansheng Ni, Sichao Jiang, Xinyu wu, Hui Shen, Yuli Zhou

**Abstract:** Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.


---

## 1. [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/abs/2311.07590) <a name="link1"></a>
**ArXiv ID:** 2311.07590
**Authors:** Jérémy Scheurer, Mikita Balesni, Marius Hobbhahn

**Abstract:** We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.


---

## 2. [A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective](https://arxiv.org/abs/2311.16065) <a name="link2"></a>
**ArXiv ID:** 2311.16065
**Authors:** Xianghua Xie, Chen Hu, Hanchi Ren, Jingjing Deng

**Abstract:** This review paper takes a comprehensive look at malicious attacks against FL,
categorizing them from new perspectives on attack origins and targets, and
providing insights into their methodology and impact. In this survey, we focus
on threat models targeting the learning process of FL systems. Based on the
source and target of the attack, we categorize existing threat models into four
types, Data to Model (D2M), Model to Data (M2D), Model to Model (M2M) and
composite attacks. For each attack type, we discuss the defense strategies
proposed, highlighting their effectiveness, assumptions and potential areas for
improvement. Defense strategies have evolved from using a singular metric to
excluding malicious clients, to employing a multifaceted approach examining
client models at various phases. In this survey paper, our research indicates
that the to-learn data, the learning gradients, and the learned model at
different stages all can be manipulated to initiate malicious attacks that
range from undermining model performance, reconstructing private local data,
and to inserting backdoors. We have also seen these threat are becoming more
insidious. While earlier studies typically amplified malicious gradients,
recent endeavors subtly alter the least significant weights in local models to
bypass defense measures. This literature review provides a holistic
understanding of the current FL threat landscape and highlights the importance
of developing robust, efficient, and privacy-preserving defenses to ensure the
safe and trusted adoption of FL in real-world applications.


---

## 3. [AdaptGuard: Defending Against Universal Attacks for Model Adaptation](https://arxiv.org/abs/2303.10594) <a name="link3"></a>
**ArXiv ID:** 2303.10594
**Authors:** Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan

**Abstract:** Model adaptation aims at solving the domain transfer problem under the
constraint of only accessing the pretrained source models. With the increasing
considerations of data privacy and transmission efficiency, this paradigm has
been gaining recent popularity. This paper studies the vulnerability to
universal attacks transferred from the source domain during model adaptation
algorithms due to the existence of malicious providers. We explore both
universal adversarial perturbations and backdoor attacks as loopholes on the
source side and discover that they still survive in the target models after
adaptation. To address this issue, we propose a model preprocessing framework,
named AdaptGuard, to improve the security of model adaptation algorithms.
AdaptGuard avoids direct use of the risky source parameters through knowledge
distillation and utilizes the pseudo adversarial samples under adjusted radius
to enhance the robustness. AdaptGuard is a plug-and-play module that requires
neither robust pretrained models nor any changes for the following model
adaptation algorithms. Extensive results on three commonly used datasets and
two popular adaptation methods validate that AdaptGuard can effectively defend
against universal attacks and maintain clean accuracy in the target domain
simultaneously. We hope this research will shed light on the safety and
robustness of transfer learning. Code is available at
https://github.com/TomSheng21/AdaptGuard.


---

## 4. [ViT-Lens-2: Gateway to Omni-modal Intelligence](https://arxiv.org/abs/2311.16081) <a name="link4"></a>
**ArXiv ID:** 2311.16081
**Authors:** Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou

**Abstract:** Aiming to advance AI agents, large foundation models significantly improve
reasoning and instruction execution, yet the current focus on vision and
language neglects the potential of perceiving diverse modalities in open-world
environments. However, the success of data-driven vision and language models is
costly or even infeasible to be reproduced for rare modalities. In this paper,
we present ViT-Lens-2 that facilitates efficient omni-modal representation
learning by perceiving novel modalities with a pretrained ViT and aligning them
to a pre-defined space. Specifically, the modality-specific lens is tuned to
project any-modal signals to an intermediate embedding space, which are then
processed by a strong ViT with pre-trained visual knowledge. The encoded
representations are optimized toward aligning with the modal-independent space,
pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified
solution for representation learning of increasing modalities with two
appealing advantages: (i) Unlocking the great potential of pretrained ViTs to
novel modalities effectively with efficient data regime; (ii) Enabling emergent
downstream capabilities through modality alignment and shared ViT parameters.
We tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,
tactile and EEG, and set new state-of-the-art results across various
understanding tasks, such as zero-shot classification. By seamlessly
integrating ViT-Lens-2 into Multimodal Foundation Models, we enable
Any-modality to Text and Image Generation in a zero-shot manner. Code and
models are available at https://github.com/TencentARC/ViT-Lens.


---

## 5. [Utilizing Explainability Techniques for Reinforcement Learning Model Assurance](https://arxiv.org/abs/2311.15838) <a name="link5"></a>
**ArXiv ID:** 2311.15838
**Authors:** Alexander Tapley, Kyle Gatesman, Luis Robaina, Brett Bissey, Joseph Weissman

**Abstract:** Explainable Reinforcement Learning (XRL) can provide transparency into the
decision-making process of a Deep Reinforcement Learning (DRL) model and
increase user trust and adoption in real-world use cases. By utilizing XRL
techniques, researchers can identify potential vulnerabilities within a trained
DRL model prior to deployment, therefore limiting the potential for mission
failure or mistakes by the system. This paper introduces the ARLIN (Assured RL
Model Interrogation) Toolkit, an open-source Python library that identifies
potential vulnerabilities and critical points within trained DRL models through
detailed, human-interpretable explainability outputs. To illustrate ARLIN's
effectiveness, we provide explainability visualizations and vulnerability
analysis for a publicly available DRL model. The open-source code repository is
available for download at https://github.com/mitre/arlin.


---

## 6. [Forecasting Auxiliary Energy Consumption for Electric Heavy-Duty Vehicles](https://arxiv.org/abs/2311.16003) <a name="link6"></a>
**ArXiv ID:** 2311.16003
**Authors:** Yuantao Fan, Zhenkan Wang, Sepideh Pashami, Slawomir Nowaczyk, Henrik Ydreskog

**Abstract:** Accurate energy consumption prediction is crucial for optimizing the
operation of electric commercial heavy-duty vehicles, e.g., route planning for
charging. Moreover, understanding why certain predictions are cast is paramount
for such a predictive model to gain user trust and be deployed in practice.
Since commercial vehicles operate differently as transportation tasks, ambient,
and drivers vary, a heterogeneous population is expected when building an AI
system for forecasting energy consumption. The dependencies between the input
features and the target values are expected to also differ across
sub-populations. One well-known example of such a statistical phenomenon is the
Simpson paradox. In this paper, we illustrate that such a setting poses a
challenge for existing XAI methods that produce global feature statistics, e.g.
LIME or SHAP, causing them to yield misleading results. We demonstrate a
potential solution by training multiple regression models on subsets of data.
It not only leads to superior regression performance but also more relevant and
consistent LIME explanations. Given that the employed groupings correspond to
relevant sub-populations, the associations between the input features and the
target values are consistent within each cluster but different across clusters.
Experiments on both synthetic and real-world datasets show that such splitting
of a complex problem into simpler ones yields better regression performance and
interpretability.


---

## 7. [Have we built machines that think like people?](https://arxiv.org/abs/2311.16093) <a name="link7"></a>
**ArXiv ID:** 2311.16093
**Authors:** Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, Eric Schulz

**Abstract:** A chief goal of artificial intelligence is to build machines that think like
people. Yet it has been argued that deep neural network architectures fail to
accomplish this. Researchers have asserted these models' limitations in the
domains of causal reasoning, intuitive physics, and intuitive psychology. Yet
recent advancements, namely the rise of large language models, particularly
those designed for visual processing, have rekindled interest in the potential
to emulate human-like cognitive abilities. This paper evaluates the current
state of vision-based large language models in the domains of intuitive
physics, causal reasoning, and intuitive psychology. Through a series of
controlled experiments, we investigate the extent to which these modern models
grasp complex physical interactions, causal relationships, and intuitive
understanding of others' preferences. Our findings reveal that, while these
models demonstrate a notable proficiency in processing and interpreting visual
data, they still fall short of human capabilities in these areas. The models
exhibit a rudimentary understanding of physical laws and causal relationships,
but their performance is hindered by a lack of deeper insights-a key aspect of
human cognition. Furthermore, in tasks requiring an intuitive theory of mind,
the models fail altogether. Our results emphasize the need for integrating more
robust mechanisms for understanding causality, physical dynamics, and social
cognition into modern-day, vision-based language models, and point out the
importance of cognitively-inspired benchmarks.


---

## 8. [Diagnosis driven Anomaly Detection for CPS](https://arxiv.org/abs/2311.15924) <a name="link8"></a>
**ArXiv ID:** 2311.15924
**Authors:** Henrik S. Steude, Lukas Moddemann, Alexander Diedrich, Jonas Ehrhardt, Oliver Niggemann

**Abstract:** In Cyber-Physical Systems (CPS) research, anomaly detection (detecting
abnormal behavior) and diagnosis (identifying the underlying root cause) are
often treated as distinct, isolated tasks. However, diagnosis algorithms
require symptoms, i.e. temporally and spatially isolated anomalies, as input.
Thus, anomaly detection and diagnosis must be developed together to provide a
holistic solution for diagnosis in CPS. We therefore propose a method for
utilizing deep learning-based anomaly detection to generate inputs for
Consistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and
a real-world CPS dataset, where our model demonstrates strong performance
relative to other state-of-the-art models.


---

## 9. [HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](https://arxiv.org/abs/2311.12454) <a name="link9"></a>
**ArXiv ID:** 2311.12454
**Authors:** Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee

**Abstract:** Large language models (LLM)-based speech synthesis has been widely adopted in
zero-shot speech synthesis. However, they require a large-scale data and
possess the same limitations as previous autoregressive speech models,
including slow inference speed and lack of robustness. This paper proposes
HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech
(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis
frameworks could significantly improve the robustness and expressiveness of the
synthetic speech. Furthermore, we significantly improve the naturalness and
speaker similarity of synthetic speech even in zero-shot speech synthesis
scenarios. For text-to-speech, we adopt the text-to-vec framework, which
generates a self-supervised speech representation and an F0 representation
based on text representations and prosody prompts. Then, HierSpeech++ generates
speech from the generated vector, F0, and voice prompt. We further introduce a
high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The
experimental results demonstrated that the hierarchical variational autoencoder
could be a strong zero-shot speech synthesizer given that it outperforms
LLM-based and diffusion-based models. Moreover, we achieved the first
human-level quality zero-shot speech synthesis. Audio samples and source code
are available at https://github.com/sh-lee-prml/HierSpeechpp.


---

## 10. [Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence](https://arxiv.org/abs/2311.15782) <a name="link10"></a>
**ArXiv ID:** 2311.15782
**Authors:** Svetlana Pavlitska, Hannes Grolig, J. Marius Zöllner

**Abstract:** Increasing the model capacity is a known approach to enhance the adversarial
robustness of deep learning networks. On the other hand, various model
compression techniques, including pruning and quantization, can reduce the size
of the network while preserving its accuracy. Several recent studies have
addressed the relationship between model compression and adversarial
robustness, while some experiments have reported contradictory results. This
work summarizes available evidence and discusses possible explanations for the
observed effects.


---

## 11. [GloNets: Globally Connected Neural Networks](https://arxiv.org/abs/2311.15947) <a name="link11"></a>
**ArXiv ID:** 2311.15947
**Authors:** Antonio Di Cecco, Carlo Metta, Marco Fantozzi, Francesco Morandin, Maurizio Parton

**Abstract:** Deep learning architectures suffer from depth-related performance
degradation, limiting the effective depth of neural networks. Approaches like
ResNet are able to mitigate this, but they do not completely eliminate the
problem. We introduce Globally Connected Neural Networks (GloNet), a novel
architecture overcoming depth-related issues, designed to be superimposed on
any model, enhancing its depth without increasing complexity or reducing
performance. With GloNet, the network's head uniformly receives information
from all parts of the network, regardless of their level of abstraction. This
enables GloNet to self-regulate information flow during training, reducing the
influence of less effective deeper layers, and allowing for stable training
irrespective of network depth. This paper details GloNet's design, its
theoretical basis, and a comparison with existing similar architectures.
Experiments show GloNet's self-regulation ability and resilience to
depth-related learning challenges, like performance degradation. Our findings
suggest GloNet as a strong alternative to traditional architectures like
ResNets.


---

## 12. [Temporal Action Localization for Inertial-based Human Activity Recognition](https://arxiv.org/abs/2311.15831) <a name="link12"></a>
**ArXiv ID:** 2311.15831
**Authors:** Marius Bock, Michael Moeller, Kristof Van Laerhoven

**Abstract:** A persistent trend in Deep Learning has been the applicability of machine
learning concepts to other areas than originally introduced for. As of today,
state-of-the-art activity recognition from wearable sensors relies on
classifiers being trained on fixed windows of data. Contrarily, video-based
Human Activity Recognition has followed a segment-based prediction approach,
localizing activity occurrences from start to end. This paper is the first to
systematically demonstrate the applicability of state-of-the-art TAL models for
wearable Human Activity Recongition (HAR) using raw inertial data as input. Our
results show that state-of-the-art TAL models are able to outperform popular
inertial models on 4 out of 6 wearable activity recognition benchmark datasets,
with improvements ranging as much as 25% in F1-score. Introducing the TAL
community's most popular metric to inertial-based HAR, namely mean Average
Precision, our analysis shows that TAL models are able to produce more coherent
segments along with an overall higher NULL-class accuracy across all datasets.
Being the first to provide such an analysis, the TAL community offers an
interesting new perspective to inertial-based HAR with yet to be explored
design choices and training concepts, which could be of significant value for
the inertial-based HAR community.


---

## 13. [Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes](https://arxiv.org/abs/2311.15946) <a name="link13"></a>
**ArXiv ID:** 2311.15946
**Authors:** Tuan-Dung Le, Zhuqi Miao, Samuel Alvarado, Brittany Smith, William Paiva, Thanh Thieu

**Abstract:** Function is increasingly recognized as an important indicator of whole-person
health, although it receives little attention in clinical natural language
processing research. We introduce the first public annotated dataset
specifically on the Mobility domain of the International Classification of
Functioning, Disability and Health (ICF), aiming to facilitate automatic
extraction and analysis of functioning information from free-text clinical
notes. We utilize the National NLP Clinical Challenges (n2c2) research dataset
to construct a pool of candidate sentences using keyword expansion. Our active
learning approach, using query-by-committee sampling weighted by density
representativeness, selects informative sentences for human annotation. We
train BERT and CRF models, and use predictions from these models to guide the
selection of new sentences for subsequent annotation iterations. Our final
dataset consists of 4,265 sentences with a total of 11,784 entities, including
5,511 Action entities, 5,328 Mobility entities, 306 Assistance entities, and
639 Quantification entities. The inter-annotator agreement (IAA), averaged over
all entity types, is 0.72 for exact matching and 0.91 for partial matching. We
also train and evaluate common BERT models and state-of-the-art Nested NER
models. The best F1 scores are 0.84 for Action, 0.7 for Mobility, 0.62 for
Assistance, and 0.71 for Quantification. Empirical results demonstrate
promising potential of NER models to accurately extract mobility functioning
information from clinical text. The public availability of our annotated
dataset will facilitate further research to comprehensively capture functioning
information in electronic health records (EHRs).


---

## 14. [TorchRL: A data-driven decision-making library for PyTorch](https://arxiv.org/abs/2306.00577) <a name="link14"></a>
**ArXiv ID:** 2306.00577
**Authors:** Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, Vincent Moens

**Abstract:** PyTorch has ascended as a premier machine learning framework, yet it lacks a
native and comprehensive library for decision and control tasks suitable for
large development teams dealing with complex real-world data and environments.
To address this issue, we propose TorchRL, a generalistic control library for
PyTorch that provides well-integrated, yet standalone components. We introduce
a new and flexible PyTorch primitive, the TensorDict, which facilitates
streamlined algorithm development across the many branches of Reinforcement
Learning (RL) and control. We provide a detailed description of the building
blocks and an extensive overview of the library across domains and tasks.
Finally, we experimentally demonstrate its reliability and flexibility and show
comparative benchmarks to demonstrate its computational efficiency. TorchRL
fosters long-term support and is publicly available on GitHub for greater
reproducibility and collaboration within the research community. The code is
open-sourced on GitHub.


---

## 15. [Understanding plasticity in neural networks](https://arxiv.org/abs/2303.01486) <a name="link15"></a>
**ArXiv ID:** 2303.01486
**Authors:** Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, Will Dabney

**Abstract:** Plasticity, the ability of a neural network to quickly change its predictions
in response to new information, is essential for the adaptability and
robustness of deep reinforcement learning systems. Deep neural networks are
known to lose plasticity over the course of training even in relatively simple
learning problems, but the mechanisms driving this phenomenon are still poorly
understood. This paper conducts a systematic empirical analysis into plasticity
loss, with the goal of understanding the phenomenon mechanistically in order to
guide the future development of targeted solutions. We find that loss of
plasticity is deeply connected to changes in the curvature of the loss
landscape, but that it often occurs in the absence of saturated units. Based on
this insight, we identify a number of parameterization and optimization design
choices which enable networks to better preserve plasticity over the course of
training. We validate the utility of these findings on larger-scale RL
benchmarks in the Arcade Learning Environment.


---

## 16. [Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models](https://arxiv.org/abs/2311.16017) <a name="link16"></a>
**ArXiv ID:** 2311.16017
**Authors:** Stephen MacNeil, Paul Denny, Andrew Tran, Juho Leinonen, Seth Bernstein, Arto Hellas, Sami Sarsa, Joanne Kim

**Abstract:** Identifying and resolving logic errors can be one of the most frustrating
challenges for novices programmers. Unlike syntax errors, for which a compiler
or interpreter can issue a message, logic errors can be subtle. In certain
conditions, buggy code may even exhibit correct behavior -- in other cases, the
issue might be about how a problem statement has been interpreted. Such errors
can be hard to spot when reading the code, and they can also at times be missed
by automated tests. There is great educational potential in automatically
detecting logic errors, especially when paired with suitable feedback for
novices. Large language models (LLMs) have recently demonstrated surprising
performance for a range of computing tasks, including generating and explaining
code. These capabilities are closely linked to code syntax, which aligns with
the next token prediction behavior of LLMs. On the other hand, logic errors
relate to the runtime performance of code and thus may not be as well suited to
analysis by LLMs. To explore this, we investigate the performance of two
popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly
explanation of logic errors. We compare LLM performance with a large cohort of
introductory computing students $(n=964)$ solving the same error detection
task. Through a mixed-methods analysis of student and model responses, we
observe significant improvement in logic error identification between the
previous and current generation of LLMs, and find that both LLM generations
significantly outperform students. We outline how such models could be
integrated into computing education tools, and discuss their potential for
supporting students when learning programming.


---

## 17. [Energy Discrepancies: A Score-Independent Loss for Energy-Based Models](https://arxiv.org/abs/2307.06431) <a name="link17"></a>
**ArXiv ID:** 2307.06431
**Authors:** Tobias Schröder, Zijing Ou, Jen Ning Lim, Yingzhen Li, Sebastian J. Vollmer, Andrew B. Duncan

**Abstract:** Energy-based models are a simple yet powerful class of probabilistic models,
but their widespread adoption has been limited by the computational burden of
training them. We propose a novel loss function called Energy Discrepancy (ED)
which does not rely on the computation of scores or expensive Markov chain
Monte Carlo. We show that ED approaches the explicit score matching and
negative log-likelihood loss under different limits, effectively interpolating
between both. Consequently, minimum ED estimation overcomes the problem of
nearsightedness encountered in score-based estimation methods, while also
enjoying theoretical guarantees. Through numerical experiments, we demonstrate
that ED learns low-dimensional data distributions faster and more accurately
than explicit score matching or contrastive divergence. For high-dimensional
image data, we describe how the manifold hypothesis puts limitations on our
approach and demonstrate the effectiveness of energy discrepancy by training
the energy-based model as a prior of a variational decoder model.


---

## 18. [Metric Space Magnitude for Evaluating Unsupervised Representation Learning](https://arxiv.org/abs/2311.16054) <a name="link18"></a>
**ArXiv ID:** 2311.16054
**Authors:** Katharina Limbeck, Rayna Andreeva, Rik Sarkar, Bastian Rieck

**Abstract:** The magnitude of a metric space was recently established as a novel
invariant, providing a measure of the `effective size' of a space across
multiple scales. By capturing both geometrical and topological properties of
data, magnitude is poised to address challenges in unsupervised representation
learning tasks. We formalise a novel notion of dissimilarity between magnitude
functions of finite metric spaces and use them to derive a quality measure for
dimensionality reduction tasks. Our measure is provably stable under
perturbations of the data, can be efficiently calculated, and enables a
rigorous multi-scale comparison of embeddings. We show the utility of our
measure in an experimental suite that comprises different domains and tasks,
including the comparison of data visualisations.


---

## 19. [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101) <a name="link19"></a>
**ArXiv ID:** 2311.16101
**Authors:** Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie

**Abstract:** This work focuses on the potential of Vision LLMs (VLLMs) in visual
reasoning. Different from prior studies, we shift our focus from evaluating
standard performance to introducing a comprehensive safety evaluation suite,
covering both out-of-distribution (OOD) generalization and adversarial
robustness. For the OOD evaluation, we present two novel VQA datasets, each
with one variant, designed to test model performance under challenging
conditions. In exploring adversarial robustness, we propose a straightforward
attack strategy for misleading VLLMs to produce visual-unrelated responses.
Moreover, we assess the efficacy of two jailbreaking strategies, targeting
either the vision or language component of VLLMs. Our evaluation of 21 diverse
models, ranging from open-source VLLMs to GPT-4V, yields interesting
observations: 1) Current VLLMs struggle with OOD texts but not images, unless
the visual information is limited; and 2) These VLLMs can be easily misled by
deceiving vision encoders only, and their vision-language training often
compromise safety protocols. We release this safety evaluation suite at
https://github.com/UCSC-VLAA/vllm-safety-benchmark.


---

## 20. [PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators](https://arxiv.org/abs/2310.19106) <a name="link20"></a>
**ArXiv ID:** 2310.19106
**Authors:** Antonin Sulc, Raimund Kammering, Annika Eichler, Tim Wilksen

**Abstract:** Navigating the landscape of particle accelerators has become increasingly
challenging with recent surges in contributions. These intricate devices
challenge comprehension, even within individual facilities. To address this, we
introduce PACuna, a fine-tuned language model refined through publicly
available accelerator resources like conferences, pre-prints, and books. We
automated data collection and question generation to minimize expert
involvement and make the data publicly available. PACuna demonstrates
proficiency in addressing intricate accelerator questions, validated by
experts. Our approach shows adapting language models to scientific domains by
fine-tuning technical texts and auto-generated corpora capturing the latest
developments can further produce pre-trained models to answer some intricate
questions that commercially available assistants cannot and can serve as
intelligent assistants for individual facilities.


---

## 21. [DeepTSF: Codeless machine learning operations for time series forecasting](https://arxiv.org/abs/2308.00709) <a name="link21"></a>
**ArXiv ID:** 2308.00709
**Authors:** Sotiris Pelekis, Evangelos Karakolis, Theodosios Pountridis, George Kormpakis, George Lampropoulos, Spiros Mouzakitis, Dimitris Askounis

**Abstract:** This paper presents DeepTSF, a comprehensive machine learning operations
(MLOps) framework aiming to innovate time series forecasting through workflow
automation and codeless modeling. DeepTSF automates key aspects of the ML
lifecycle, making it an ideal tool for data scientists and MLops engineers
engaged in machine learning (ML) and deep learning (DL)-based forecasting.
DeepTSF empowers users with a robust and user-friendly solution, while it is
designed to seamlessly integrate with existing data analysis workflows,
providing enhanced productivity and compatibility. The framework offers a
front-end user interface (UI) suitable for data scientists, as well as other
higher-level stakeholders, enabling comprehensive understanding through
insightful visualizations and evaluation metrics. DeepTSF also prioritizes
security through identity management and access authorization mechanisms. The
application of DeepTSF in real-life use cases of the I-NERGY project has
already proven DeepTSF's efficacy in DL-based load forecasting, showcasing its
significant added value in the electrical power and energy systems domain.


---

## 22. [Data Generation for Post-OCR correction of Cyrillic handwriting](https://arxiv.org/abs/2311.15896) <a name="link22"></a>
**ArXiv ID:** 2311.15896
**Authors:** Evgenii Davydkin, Aleksandr Markelov, Egor Iuldashev, Anton Dudkin, Ivan Krivorotov

**Abstract:** This paper introduces a novel approach to post-Optical Character Recognition
Correction (POC) for handwritten Cyrillic text, addressing a significant gap in
current research methodologies. This gap is due to the lack of large text
corporas that provide OCR errors for further training of language-based POC
models, which are demanding in terms of corpora size. Our study primarily
focuses on the development and application of a synthetic handwriting
generation engine based on B\'ezier curves. Such an engine generates highly
realistic handwritten text in any amounts, which we utilize to create a
substantial dataset by transforming Russian text corpora sourced from the
internet. We apply a Handwritten Text Recognition (HTR) model to this dataset
to identify OCR errors, forming the basis for our POC model training. The
correction model is trained on a 90-symbol input context, utilizing a
pre-trained T5 architecture with a seq2seq correction task. We evaluate our
approach on HWR200 and School_notebooks_RU datasets as they provide significant
challenges in the HTR domain. Furthermore, POC can be used to highlight errors
for teachers, evaluating student performance. This can be done simply by
comparing sentences before and after correction, displaying differences in
text. Our primary contribution lies in the innovative use of B\'ezier curves
for Cyrillic text generation and subsequent error correction using a
specialized POC model. We validate our approach by presenting Word Accuracy
Rate (WAR) and Character Accuracy Rate (CAR) results, both with and without
post-OCR correction, using real open corporas of handwritten Cyrillic text.
These results, coupled with our methodology, are designed to be reproducible,
paving the way for further advancements in the field of OCR and handwritten
text analysis. Paper contributions can be found in
https://github.com/dbrainio/CyrillicHandwritingPOC


---

## 23. [Nova$^+$: Generative Language Models for Binaries](https://arxiv.org/abs/2311.13721) <a name="link23"></a>
**ArXiv ID:** 2311.13721
**Authors:** Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang

**Abstract:** Generative large language models (LLMs) pre-trained on code have shown
impressive effectiveness in code generation, program repair, and document
analysis. However, existing generative LLMs focus on source code and are not
specialized for binaries. There are three main challenges for LLMs to model and
learn binary code: hex-decimal values, complex global dependencies, and
compiler optimization levels. To bring the benefit of LLMs to the binary
domain, we develop Nova and Nova$^+$, which are LLMs pre-trained on binary
corpora. Nova is pre-trained with the standard language modeling task, showing
significantly better capability on five benchmarks for three downstream tasks:
binary code similarity detection (BCSD), binary code translation (BCT), and
binary code recovery (BCR), over GPT-3.5 and other existing techniques. We
build Nova$^+$ to further boost Nova using two new pre-training tasks, i.e.,
optimization generation and optimization level prediction, which are designed
to learn binary optimization and align equivalent binaries. Nova$^+$ shows
overall the best performance for all three downstream tasks on five benchmarks,
demonstrating the contributions of the new pre-training tasks.


---

## 24. [Assessing Deep Neural Networks as Probability Estimators](https://arxiv.org/abs/2111.08239) <a name="link24"></a>
**ArXiv ID:** 2111.08239
**Authors:** Yu Pan, Kwo-Sen Kuo, Michael L. Rilee, Hongfeng Yu

**Abstract:** Deep Neural Networks (DNNs) have performed admirably in classification tasks.
However, the characterization of their classification uncertainties, required
for certain applications, has been lacking. In this work, we investigate the
issue by assessing DNNs' ability to estimate conditional probabilities and
propose a framework for systematic uncertainty characterization. Denoting the
input sample as x and the category as y, the classification task of assigning a
category y to a given input x can be reduced to the task of estimating the
conditional probabilities p(y|x), as approximated by the DNN at its last layer
using the softmax function. Since softmax yields a vector whose elements all
fall in the interval (0, 1) and sum to 1, it suggests a probabilistic
interpretation to the DNN's outcome. Using synthetic and real-world datasets,
we look into the impact of various factors, e.g., probability density f(x) and
inter-categorical sparsity, on the precision of DNNs' estimations of p(y|x),
and find that the likelihood probability density and the inter-categorical
sparsity have greater impacts than the prior probability to DNNs'
classification uncertainty.


---

## 25. [Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation](https://arxiv.org/abs/2311.16091) <a name="link25"></a>
**ArXiv ID:** 2311.16091
**Authors:** Jiachen Li, David Isele, Kanghoon Lee, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer

**Abstract:** Deep reinforcement learning (DRL) provides a promising way for intelligent
agents (e.g., autonomous vehicles) to learn to navigate complex scenarios.
However, DRL with neural networks as function approximators is typically
considered a black box with little explainability and often suffers from
suboptimal performance, especially for autonomous navigation in highly
interactive multi-agent environments. To address these issues, we propose three
auxiliary tasks with spatio-temporal relational reasoning and integrate them
into the standard DRL framework, which improves the decision making performance
and provides explainable intermediate indicators. We propose to explicitly
infer the internal states (i.e., traits and intentions) of surrounding agents
(e.g., human drivers) as well as to predict their future trajectories in the
situations with and without the ego agent through counterfactual reasoning.
These auxiliary tasks provide additional supervision signals to infer the
behavior patterns of other interactive agents. Multiple variants of framework
integration strategies are compared. We also employ a spatio-temporal graph
neural network to encode relations between dynamic entities, which enhances
both internal state inference and decision making of the ego agent. Moreover,
we propose an interactivity estimation mechanism based on the difference
between predicted trajectories in these two situations, which indicates the
degree of influence of the ego agent on other agents. To validate the proposed
method, we design an intersection driving simulator based on the Intelligent
Intersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our
approach achieves robust and state-of-the-art performance in terms of standard
evaluation metrics and provides explainable intermediate indicators (i.e.,
internal states, and interactivity scores) for decision making.


---

## 26. [From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding](https://arxiv.org/abs/2304.00553) <a name="link26"></a>
**ArXiv ID:** 2304.00553
**Authors:** Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, Cewu Lu

**Abstract:** As a vital step toward the intelligent agent, Action understanding matters
for intelligent agents and has attracted long-term attention. It can be formed
as the mapping from the action physical space to the semantic space. Typically,
researchers built action datasets according to idiosyncratic choices to define
classes and push the envelope of benchmarks respectively. Thus, datasets are
incompatible with each other like "Isolated Islands" due to semantic gaps and
various class granularities, e.g., do housework in dataset A and wash plate in
dataset B. We argue that a more principled semantic space is an urgent need to
concentrate the community efforts and enable us to use all datasets together to
pursue generalizable action learning. To this end, we design a structured
action semantic space in view of verb taxonomy hierarchy and covering massive
actions. By aligning the classes of previous datasets to our semantic space, we
gather (image/video/skeleton/MoCap) datasets into a unified database in a
unified label system, i.e., bridging ``isolated islands'' into a "Pangea".
Accordingly, we propose a novel model mapping from the physical space to
semantic space to fully use Pangea. In extensive experiments, our new system
shows significant superiority, especially in transfer learning. Code and data
will be made publicly available.


---

## 27. [RIDE: Real-time Intrusion Detection via Explainable Machine Learning Implemented in a Memristor Hardware Architecture](https://arxiv.org/abs/2311.16018) <a name="link27"></a>
**ArXiv ID:** 2311.16018
**Authors:** Jingdi Chen, Lei Zhang, Joseph Riem, Gina Adam, Nathaniel D. Bastian, Tian Lan

**Abstract:** Deep Learning (DL) based methods have shown great promise in network
intrusion detection by identifying malicious network traffic behavior patterns
with high accuracy, but their applications to real-time, packet-level
detections in high-speed communication networks are challenging due to the high
computation time and resource requirements of Deep Neural Networks (DNNs), as
well as lack of explainability. To this end, we propose a packet-level network
intrusion detection solution that makes novel use of Recurrent Autoencoders to
integrate an arbitrary-length sequence of packets into a more compact joint
feature embedding, which is fed into a DNN-based classifier. To enable
explainability and support real-time detections at micro-second speed, we
further develop a Software-Hardware Co-Design approach to efficiently realize
the proposed solution by converting the learned detection policies into
decision trees and implementing them using an emerging architecture based on
memristor devices. By jointly optimizing associated software and hardware
constraints, we show that our approach leads to an extremely efficient,
real-time solution with high detection accuracy at the packet level. Evaluation
results on real-world datasets (e.g., UNSW and CIC-IDS datasets) demonstrate
nearly three-nines detection accuracy with a substantial speedup of nearly four
orders of magnitude.


---

## 28. [A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems](https://arxiv.org/abs/2311.04014) <a name="link28"></a>
**ArXiv ID:** 2311.04014
**Authors:** Cheng Yin, Yi Chen

**Abstract:** This paper introduces a novel operator, termed the Y operator, to elevate
control performance in Actor-Critic(AC) based reinforcement learning for
systems governed by stochastic differential equations(SDEs). The Y operator
ingeniously integrates the stochasticity of a class of child-mother system into
the Critic network's loss function, yielding substantial advancements in the
control performance of RL algorithms.Additionally, the Y operator elegantly
reformulates the challenge of solving partial differential equations for the
state-value function into a parallel problem for the drift and diffusion
functions within the system's SDEs.A rigorous mathematical proof confirms the
operator's validity.This transformation enables the Y Operator-based
Reinforcement Learning(YORL) framework to efficiently tackle optimal control
problems in both model-based and data-driven systems.The superiority of YORL is
demonstrated through linear and nonlinear numerical examples showing its
enhanced performance over existing methods post convergence.


---

## 29. [Exploring Artificial Intelligence Methods for Energy Prediction in Healthcare Facilities: An In-Depth Extended Systematic Review](https://arxiv.org/abs/2311.15807) <a name="link29"></a>
**ArXiv ID:** 2311.15807
**Authors:** Marjan FatehiJananloo, Helen Stopps, J. J. McArthur

**Abstract:** Hospitals, due to their complexity and unique requirements, play a pivotal
role in global energy consumption patterns. This study conducted a
comprehensive literature review, utilizing the PRISMA framework, of articles
that employed machine learning and artificial intelligence techniques for
predicting energy consumption in hospital buildings. Of the 1884 publications
identified, 17 were found to address this specific domain and have been
thoroughly reviewed to establish the state-of-the-art and identify gaps where
future research is needed. This review revealed a diverse range of data inputs
influencing energy prediction, with occupancy and meteorological data emerging
as significant predictors. However, many studies failed to delve deep into the
implications of their data choices, and gaps were evident regarding the
understanding of time dynamics, operational status, and preprocessing methods.
Machine learning, especially deep learning models like ANNs, have shown
potential in this domain, yet they come with challenges, including
interpretability and computational demands. The findings underscore the immense
potential of AI in optimizing hospital energy consumption but also highlight
the need for more comprehensive and granular research. Key areas for future
research include the optimization of ANN approaches, new optimization and data
integration techniques, the integration of real-time data into Intelligent
Energy Management Systems, and increasing focus on long-term energy
forecasting.


---

## 30. [A precise symbolic emulator of the linear matter power spectrum](https://arxiv.org/abs/2311.15865) <a name="link30"></a>
**ArXiv ID:** 2311.15865
**Authors:** Deaglan J. Bartlett, Lukas Kammerer, Gabriel Kronberger, Harry Desmond, Pedro G. Ferreira, Benjamin D. Wandelt, Bogdan Burlacu, David Alonso, Matteo Zennaro

**Abstract:** Computing the matter power spectrum, $P(k)$, as a function of cosmological
parameters can be prohibitively slow in cosmological analyses, hence emulating
this calculation is desirable. Previous analytic approximations are
insufficiently accurate for modern applications, so black-box, uninterpretable
emulators are often used. We utilise an efficient genetic programming based
symbolic regression framework to explore the space of potential mathematical
expressions which can approximate the power spectrum and $\sigma_8$. We learn
the ratio between an existing low-accuracy fitting function for $P(k)$ and that
obtained by solving the Boltzmann equations and thus still incorporate the
physics which motivated this earlier approximation. We obtain an analytic
approximation to the linear power spectrum with a root mean squared fractional
error of 0.2% between $k = 9\times10^{-3} - 9 \, h{\rm \, Mpc^{-1}}$ and across
a wide range of cosmological parameters, and we provide physical
interpretations for various terms in the expression. We also provide a simple
analytic approximation for $\sigma_8$ with a similar accuracy, with a root mean
squared fractional error of just 0.4% when evaluated across the same range of
cosmologies. This function is easily invertible to obtain $A_{\rm s}$ as a
function of $\sigma_8$ and the other cosmological parameters, if preferred. It
is possible to obtain symbolic approximations to a seemingly complex function
at a precision required for current and future cosmological analyses without
resorting to deep-learning techniques, thus avoiding their black-box nature and
large number of parameters. Our emulator will be usable long after the codes on
which numerical approximations are built become outdated.


---

## 31. [BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification](https://arxiv.org/abs/2311.16083) <a name="link31"></a>
**ArXiv ID:** 2311.16083
**Authors:** Dmitri Roussinov, Serge Sharoff

**Abstract:** While performance of many text classification tasks has been recently
improved due to Pre-trained Language Models (PLMs), in this paper we show that
they still suffer from a performance gap when the underlying distribution of
topics changes. For example, a genre classifier trained on \textit{political}
topics often fails when tested on documents about \textit{sport} or
\textit{medicine}. In this work, we quantify this phenomenon empirically with a
large corpus and a large set of topics. Consequently, we verify that domain
transfer remains challenging both for classic PLMs, such as BERT, and for
modern large models, such as GPT-3. We also suggest and successfully test a
possible remedy: after augmenting the training dataset with
topically-controlled synthetic texts, the F1 score improves by up to 50\% for
some topics, nearing on-topic training results, while others show little to no
improvement. While our empirical results focus on genre classification, our
methodology is applicable to other classification tasks such as gender,
authorship, or sentiment classification. The code and data to replicate the
experiments are available at https://github.com/dminus1/genre


---

## 32. [RCT Rejection Sampling for Causal Estimation Evaluation](https://arxiv.org/abs/2307.15176) <a name="link32"></a>
**ArXiv ID:** 2307.15176
**Authors:** Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya

**Abstract:** Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.


---

## 33. [MAST: Model-Agnostic Sparsified Training](https://arxiv.org/abs/2311.16086) <a name="link33"></a>
**ArXiv ID:** 2311.16086
**Authors:** Yury Demidovich, Grigory Malinovsky, Egor Shulgin, Peter Richtárik

**Abstract:** We introduce a novel optimization problem formulation that departs from the
conventional way of minimizing machine learning model loss as a black-box
function. Unlike traditional formulations, the proposed approach explicitly
incorporates an initially pre-trained model and random sketch operators,
allowing for sparsification of both the model and gradient during training. We
establish insightful properties of the proposed objective function and
highlight its connections to the standard formulation. Furthermore, we present
several variants of the Stochastic Gradient Descent (SGD) method adapted to the
new problem formulation, including SGD with general sampling, a distributed
version, and SGD with variance reduction techniques. We achieve tighter
convergence rates and relax assumptions, bridging the gap between theoretical
principles and practical applications, covering several important techniques
such as Dropout and Sparse training. This work presents promising opportunities
to enhance the theoretical understanding of model training through a
sparsification-aware optimization approach.


---

## 34. [Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs](https://arxiv.org/abs/2311.15781) <a name="link34"></a>
**ArXiv ID:** 2311.15781
**Authors:** Simone Conia, Min Li, Daniel Lee, Umar Farooq Minhas, Ihab Ilyas, Yunyao Li

**Abstract:** Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.


---

## 35. [Sentiment analysis with adaptive multi-head attention in Transformer](https://arxiv.org/abs/2310.14505) <a name="link35"></a>
**ArXiv ID:** 2310.14505
**Authors:** Fanfei Meng, David Demeter

**Abstract:** We propose a novel framework based on the attention mechanism to identify the
sentiment of a movie review document. Previous efforts on deep neural networks
with attention mechanisms focus on encoder and decoder with fixed numbers of
multi-head attention. Therefore, we need a mechanism to stop the attention
process automatically if no more useful information can be read from the
memory.In this paper, we propose an adaptive multi-head attention architecture
(AdaptAttn) which varies the number of attention heads based on length of
sentences. AdaptAttn has a data preprocessing step where each document is
classified into any one of the three bins small, medium or large based on
length of the sentence. The document classified as small goes through two heads
in each layer, the medium group passes four heads and the large group is
processed by eight heads. We examine the merit of our model on the Stanford
large movie review dataset. The experimental results show that the F1 score
from our model is on par with the baseline model.


---

## 36. [Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning](https://arxiv.org/abs/2102.12920) <a name="link36"></a>
**ArXiv ID:** 2102.12920
**Authors:** Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid

**Abstract:** Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. This survey reviews the state of the art, challenges,
and future directions.


---

## 37. [Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers](https://arxiv.org/abs/2311.15983) <a name="link37"></a>
**ArXiv ID:** 2311.15983
**Authors:** Yilun Liu, Difan Jiao, Ashton Anderson

**Abstract:** Among the many tasks that Large Language Models (LLMs) have revolutionized is
text classification. However, existing approaches for applying pretrained LLMs
to text classification predominantly rely on using single token outputs from
only the last layer of hidden states. As a result, they suffer from limitations
in efficiency, task-specificity, and interpretability. In our work, we
contribute an approach that uses all internal representations by employing
multiple pooling strategies on all activation and hidden states. Our novel
lightweight strategy, Sparsify-then-Classify (STC) first sparsifies
task-specific features layer-by-layer, then aggregates across layers for text
classification. STC can be applied as a seamless plug-and-play module on top of
existing LLMs. Our experiments on a comprehensive set of models and datasets
demonstrate that STC not only consistently improves the classification
performance of pretrained and fine-tuned models, but is also more efficient for
both training and inference, and is more intrinsically interpretable.


---

## 38. [Rethinking Privacy in Machine Learning Pipelines from an Information Flow Control Perspective](https://arxiv.org/abs/2311.15792) <a name="link38"></a>
**ArXiv ID:** 2311.15792
**Authors:** Lukas Wutschitz, Boris Köpf, Andrew Paverd, Saravan Rajmohan, Ahmed Salem, Shruti Tople, Santiago Zanella-Béguelin, Menglin Xia, Victor Rühle

**Abstract:** Modern machine learning systems use models trained on ever-growing corpora.
Typically, metadata such as ownership, access control, or licensing information
is ignored during training. Instead, to mitigate privacy risks, we rely on
generic techniques such as dataset sanitization and differentially private
model training, with inherent privacy/utility trade-offs that hurt model
performance. Moreover, these techniques have limitations in scenarios where
sensitive information is shared across multiple participants and fine-grained
access control is required. By ignoring metadata, we therefore miss an
opportunity to better address security, privacy, and confidentiality
challenges. In this paper, we take an information flow control perspective to
describe machine learning systems, which allows us to leverage metadata such as
access control policies and define clear-cut privacy and confidentiality
guarantees with interpretable information flows. Under this perspective, we
contrast two different approaches to achieve user-level non-interference: 1)
fine-tuning per-user models, and 2) retrieval augmented models that access
user-specific datasets at inference time. We compare these two approaches to a
trivially non-interfering zero-shot baseline using a public model and to a
baseline that fine-tunes this model on the whole corpus. We evaluate trained
models on two datasets of scientific articles and demonstrate that retrieval
augmented architectures deliver the best utility, scalability, and flexibility
while satisfying strict non-interference guarantees.


---

## 39. [What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models](https://arxiv.org/abs/2310.06627) <a name="link39"></a>
**ArXiv ID:** 2310.06627
**Authors:** Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao

**Abstract:** Counterfactual reasoning, a fundamental aspect of human cognition, involves
contemplating alternatives to established facts or past events, significantly
enhancing our abilities in planning and decision-making. In light of the
advancements in current multi-modal large language models, we explore their
effectiveness in counterfactual reasoning. To facilitate this investigation, we
introduce a novel dataset, C-VQA, specifically designed to test the
counterfactual reasoning capabilities of modern multi-modal large language
models. This dataset is constructed by infusing original questions with
counterfactual presuppositions, spanning various types such as numerical and
boolean queries. It encompasses a mix of real and synthetic data, representing
a wide range of difficulty levels. Our thorough evaluations of contemporary
vision-language models using this dataset have revealed substantial performance
drops, with some models showing up to a 40\% decrease, highlighting a
significant gap between current models and human-like vision reasoning
capabilities. We hope our dataset will serve as a vital benchmark for
evaluating the counterfactual reasoning capabilities of models. Code and
dataset are publicly available at https://bzhao.me/C-VQA/.


---

## 40. [A systematic study comparing hyperparameter optimization engines on tabular data](https://arxiv.org/abs/2311.15854) <a name="link40"></a>
**ArXiv ID:** 2311.15854
**Authors:** Balazs Kegl

**Abstract:** We run an independent comparison of all hyperparameter optimization
(hyperopt) engines available in the Ray Tune library. We introduce two ways to
normalize and aggregate statistics across data sets and models, one rank-based,
and another one sandwiching the score between the random search score and the
full grid search score. This affords us i) to rank the hyperopt engines, ii) to
make generalized and statistically significant statements on how much they
improve over random search, and iii) to make recommendations on which engine
should be used to hyperopt a given learning algorithm. We find that most
engines beat random search, but that only three of them (HEBO, AX, and
BlendSearch) clearly stand out. We also found that some engines seem to
specialize in hyperopting certain learning algorithms, which makes it tricky to
use hyperopt in comparison studies, since the choice of the hyperopt technique
may favor some of the models in the comparison.


---

## 41. [Attend Who is Weak: Enhancing Graph Condensation via Cross-Free Adversarial Training](https://arxiv.org/abs/2311.15772) <a name="link41"></a>
**ArXiv ID:** 2311.15772
**Authors:** Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, Di Wu

**Abstract:** In this paper, we study the \textit{graph condensation} problem by
compressing the large, complex graph into a concise, synthetic representation
that preserves the most essential and discriminative information of structure
and features. We seminally propose the concept of Shock Absorber (a type of
perturbation) that enhances the robustness and stability of the original graphs
against changes in an adversarial training fashion. Concretely, (I) we forcibly
match the gradients between pre-selected graph neural networks (GNNs) trained
on a synthetic, simplified graph and the original training graph at regularly
spaced intervals. (II) Before each update synthetic graph point, a Shock
Absorber serves as a gradient attacker to maximize the distance between the
synthetic dataset and the original graph by selectively perturbing the parts
that are underrepresented or insufficiently informative. We iteratively repeat
the above two processes (I and II) in an adversarial training fashion to
maintain the highly-informative context without losing correlation with the
original dataset. More importantly, our shock absorber and the synthesized
graph parallelly share the backward process in a free training manner. Compared
to the original adversarial training, it introduces almost no additional time
overhead.
  We validate our framework across 8 datasets (3 graph and 5 node
classification datasets) and achieve prominent results: for example, on Cora,
Citeseer and Ogbn-Arxiv, we can gain nearly 1.13% to 5.03% improvements compare
with SOTA models. Moreover, our algorithm adds only about 0.2% to 2.2%
additional time overhead over Flicker, Citeseer and Ogbn-Arxiv. Compared to the
general adversarial training, our approach improves time efficiency by nearly
4-fold.


---

## 42. [Self-Guided Diffusion Models](https://arxiv.org/abs/2210.06462) <a name="link42"></a>
**ArXiv ID:** 2210.06462
**Authors:** Vincent Tao Hu, David W Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek

**Abstract:** Diffusion models have demonstrated remarkable progress in image generation
quality, especially when guidance is used to control the generative process.
However, guidance requires a large amount of image-annotation pairs for
training and is thus dependent on their availability, correctness and
unbiasedness. In this paper, we eliminate the need for such annotation by
instead leveraging the flexibility of self-supervision signals to design a
framework for self-guided diffusion models. By leveraging a feature extraction
function and a self-annotation function, our method provides guidance signals
at various image granularities: from the level of holistic images to object
boxes and even segmentation masks. Our experiments on single-label and
multi-label image datasets demonstrate that self-labeled guidance always
outperforms diffusion models without guidance and may even surpass guidance
based on ground-truth labels, especially on unbalanced data. When equipped with
self-supervised box or mask proposals, our method further generates visually
diverse yet semantically consistent images, without the need for any class,
box, or segment label annotation. Self-guided diffusion is simple, flexible and
expected to profit from deployment at scale. Source code will be at:
https://taohu.me/sgdm/


---

## 43. [Long-Range Neural Atom Learning for Molecular Graphs](https://arxiv.org/abs/2311.01276) <a name="link43"></a>
**ArXiv ID:** 2311.01276
**Authors:** Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han

**Abstract:** Graph Neural Networks (GNNs) have been widely adopted for drug discovery with
molecular graphs. Nevertheless, current GNNs are mainly good at leveraging
short-range interactions (SRI) but struggle to capture long-range interactions
(LRI), both of which are crucial for determining molecular properties. To
tackle this issue, we propose a method that implicitly projects all original
atoms into a few Neural Atoms, which abstracts the collective information of
atomic groups within a molecule. Specifically, we explicitly exchange the
information among neural atoms and project them back to the atoms'
representations as an enhancement. With this mechanism, neural atoms establish
the communication channels among distant nodes, effectively reducing the
interaction scope of arbitrary node pairs into a single hop. To provide an
inspection of our method from a physical perspective, we reveal its connection
with the traditional LRI calculation method, Ewald Summation. We conduct
extensive experiments on three long-range graph benchmarks, covering both
graph-level and link-level tasks on molecular graphs. We empirically justify
that our method can be equipped with an arbitrary GNN and help to capture LRI.


---

## 44. [The Chosen One: Consistent Characters in Text-to-Image Diffusion Models](https://arxiv.org/abs/2311.10093) <a name="link44"></a>
**ArXiv ID:** 2311.10093
**Authors:** Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski

**Abstract:** Recent advances in text-to-image generation models have unlocked vast
potential for visual creativity. However, these models struggle with generation
of consistent characters, a crucial aspect for numerous real-world applications
such as story visualization, game development asset design, advertising, and
more. Current methods typically rely on multiple pre-existing images of the
target character or involve labor-intensive manual processes. In this work, we
propose a fully automated solution for consistent character generation, with
the sole input being a text prompt. We introduce an iterative procedure that,
at each stage, identifies a coherent set of images sharing a similar identity
and extracts a more consistent identity from this set. Our quantitative
analysis demonstrates that our method strikes a better balance between prompt
alignment and identity consistency compared to the baseline methods, and these
findings are reinforced by a user study. To conclude, we showcase several
practical applications of our approach. Project page is available at
https://omriavrahami.com/the-chosen-one


---

## 45. [Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges](https://arxiv.org/abs/2311.15766) <a name="link45"></a>
**ArXiv ID:** 2311.15766
**Authors:** Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang

**Abstract:** In recent years, large language models (LLMs) have spurred a new research
paradigm in natural language processing. Despite their excellent capability in
knowledge-based question answering and reasoning, their potential to retain
faulty or even harmful knowledge poses risks of malicious application. The
challenge of mitigating this issue and transforming these models into purer
assistants is crucial for their widespread applicability. Unfortunately,
Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical
due to their immense parameters. Knowledge unlearning, derived from analogous
studies on machine unlearning, presents a promising avenue to address this
concern and is notably advantageous in the context of LLMs. It allows for the
removal of harmful knowledge in an efficient manner, without affecting
unrelated knowledge in the model. To this end, we provide a survey of knowledge
unlearning in the era of LLMs. Firstly, we formally define the knowledge
unlearning problem and distinguish it from related works. Subsequently, we
categorize existing knowledge unlearning methods into three classes: those
based on parameter optimization, parameter merging, and in-context learning,
and introduce details of these unlearning methods. We further present
evaluation datasets used in existing methods, and finally conclude this survey
by presenting the ongoing challenges and future directions.


---

## 46. [Auto-CsiNet: Scenario-customized Automatic Neural Network Architecture Generation for Massive MIMO CSI Feedback](https://arxiv.org/abs/2311.15950) <a name="link46"></a>
**ArXiv ID:** 2311.15950
**Authors:** Xiangyi Li, Jiajia Guo, Chao-Kai Wen, Shi Jin

**Abstract:** Deep learning has revolutionized the design of the channel state information
(CSI) feedback module in wireless communications. However, designing the
optimal neural network (NN) architecture for CSI feedback can be a laborious
and time-consuming process. Manual design can be prohibitively expensive for
customizing NNs to different scenarios. This paper proposes using neural
architecture search (NAS) to automate the generation of scenario-customized CSI
feedback NN architectures, thereby maximizing the potential of deep learning in
exclusive environments. By employing automated machine learning and
gradient-descent-based NAS, an efficient and cost-effective architecture design
process is achieved. The proposed approach leverages implicit scene knowledge,
integrating it into the scenario customization process in a data-driven manner,
and fully exploits the potential of deep learning for each specific scenario.
To address the issue of excessive search, early stopping and elastic selection
mechanisms are employed, enhancing the efficiency of the proposed scheme. The
experimental results demonstrate that the automatically generated architecture,
known as Auto-CsiNet, outperforms manually-designed models in both
reconstruction performance (achieving approximately a 14% improvement) and
complexity (reducing it by approximately 50%). Furthermore, the paper analyzes
the impact of the scenario on the NN architecture and its capacity.


---

## 47. [Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs](https://arxiv.org/abs/2311.15759) <a name="link47"></a>
**ArXiv ID:** 2311.15759
**Authors:** Yunxin Li, Baotian Hu, Wei Wang, Xiaochun Cao, Min Zhang

**Abstract:** Recent advancements in multimodal large language models (MLLMs) have achieved
significant multimodal generation capabilities, akin to GPT-4. These models
predominantly map visual information into language representation space,
leveraging the vast knowledge and powerful text generation abilities of LLMs to
produce multimodal instruction-following responses. We could term this method
as LLMs for Vision because of its employing LLMs for visual-language
understanding, yet observe that these MLLMs neglect the potential of harnessing
visual knowledge to enhance overall capabilities of LLMs, which could be
regraded as Vision Enhancing LLMs. In this paper, we propose an approach called
MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage
and Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a
component integrated into the internal blocks of LLMs, designed to store
open-world visual information efficiently. Additionally, we present a soft
Mixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal
knowledge collaboration during generation. Our comprehensive experiments
demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs
in contexts necessitating physical or commonsense knowledge. It also delivers
competitive results on multimodal benchmarks.


---

## 48. [Learning Multi-Frequency Partial Correlation Graphs](https://arxiv.org/abs/2311.15756) <a name="link48"></a>
**ArXiv ID:** 2311.15756
**Authors:** Gabriele D'Acunto, Paolo Di Lorenzo, Francesco Bonchi, Stefania Sardellitti, Sergio Barbarossa

**Abstract:** Despite the large research effort devoted to learning dependencies between
time series, the state of the art still faces a major limitation: existing
methods learn partial correlations but fail to discriminate across distinct
frequency bands. Motivated by many applications in which this differentiation
is pivotal, we overcome this limitation by learning a block-sparse,
frequency-dependent, partial correlation graph, in which layers correspond to
different frequency bands, and partial correlations can occur over just a few
layers. To this aim, we formulate and solve two nonconvex learning problems:
the first has a closed-form solution and is suitable when there is prior
knowledge about the number of partial correlations; the second hinges on an
iterative solution based on successive convex approximation, and is effective
for the general case where no prior knowledge is available. Numerical results
on synthetic data show that the proposed methods outperform the current state
of the art. Finally, the analysis of financial time series confirms that
partial correlations exist only within a few frequency bands, underscoring how
our methods enable the gaining of valuable insights that would be undetected
without discriminating along the frequency domain.


---

## 49. [Scheduling and Communication Schemes for Decentralized Federated Learning](https://arxiv.org/abs/2311.16021) <a name="link49"></a>
**ArXiv ID:** 2311.16021
**Authors:** Bahaa-Eldin Ali Abdelghany, Ana Fernández-Vilas, Manuel Fernández-Veiga, Nashwa El-Bendary, Ammar M. Hassan, Walid M. Abdelmoez

**Abstract:** Federated learning (FL) is a distributed machine learning paradigm in which a
large number of clients coordinate with a central server to learn a model
without sharing their own training data. One central server is not enough, due
to problems of connectivity with clients. In this paper, a decentralized
federated learning (DFL) model with the stochastic gradient descent (SGD)
algorithm has been introduced, as a more scalable approach to improve the
learning performance in a network of agents with arbitrary topology. Three
scheduling policies for DFL have been proposed for communications between the
clients and the parallel servers, and the convergence, accuracy, and loss have
been tested in a totally decentralized mplementation of SGD. The experimental
results show that the proposed scheduling polices have an impact both on the
speed of convergence and in the final global model.


---

## 50. [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038) <a name="link50"></a>
**ArXiv ID:** 2311.16038
**Authors:** Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, Jiwen Lu

**Abstract:** Understanding how the 3D scene evolves is vital for making decisions in
autonomous driving. Most existing methods achieve this by predicting the
movements of object boxes, which cannot capture more fine-grained scene
information. In this paper, we explore a new framework of learning a world
model, OccWorld, in the 3D Occupancy space to simultaneously predict the
movement of the ego car and the evolution of the surrounding scenes. We propose
to learn a world model based on 3D occupancy rather than 3D bounding boxes and
segmentation maps for three reasons: 1) expressiveness. 3D occupancy can
describe the more fine-grained 3D structure of the scene; 2) efficiency. 3D
occupancy is more economical to obtain (e.g., from sparse LiDAR points). 3)
versatility. 3D occupancy can adapt to both vision and LiDAR. To facilitate the
modeling of the world evolution, we learn a reconstruction-based scene
tokenizer on the 3D occupancy to obtain discrete scene tokens to describe the
surrounding scenes. We then adopt a GPT-like spatial-temporal generative
transformer to generate subsequent scene and ego tokens to decode the future
occupancy and ego trajectory. Extensive experiments on the widely used nuScenes
benchmark demonstrate the ability of OccWorld to effectively model the
evolution of the driving scenes. OccWorld also produces competitive planning
results without using instance and map supervision. Code:
https://github.com/wzzheng/OccWorld.


---

## 51. [An HCAI Methodological Framework: Putting It Into Action to Enable Human-Centered AI](https://arxiv.org/abs/2311.16027) <a name="link51"></a>
**ArXiv ID:** 2311.16027
**Authors:** Wei Xu, Zaifeng Gao, Marvin Dainoff

**Abstract:** Human-centered AI (HCAI), as a design philosophy, advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI technology to humans and avoid its potential
adverse effects. While HCAI has gained momentum, the lack of guidance on
methodology in its implementation makes its adoption challenging. After
assessing the needs for a methodological framework for HCAI, this paper first
proposes a comprehensive and interdisciplinary HCAI methodological framework
integrated with seven components, including design goals, design principles,
implementation approaches, design paradigms, interdisciplinary teams, methods,
and processes. THe implications of the framework are also discussed. This paper
also presents a "three-layer" approach to facilitate the implementation of the
framework. We believe the proposed framework is systematic and executable,
which can overcome the weaknesses in current frameworks and the challenges
currently faced in implementing HCAI. Thus, the framework can help put it into
action to develop, transfer, and implement HCAI in practice, eventually
enabling the design, development, and deployment of HCAI-based intelligent
systems.


---

## 52. [Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces](https://arxiv.org/abs/2211.14400) <a name="link52"></a>
**ArXiv ID:** 2211.14400
**Authors:** Jonathan W. Siegel

**Abstract:** Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the
problem of how efficiently, in terms of the number of parameters, deep neural
networks with the ReLU activation function can approximate functions in the
Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with
error measured in the $L_p(\Omega)$ norm. This problem is important when
studying the application of neural networks in a variety of fields, including
scientific computing and signal processing, and has previously been solved only
when $p=q=\infty$. Our contribution is to provide a complete solution for all
$1\leq p,q\leq \infty$ and $s > 0$ for which the corresponding Sobolev or Besov
space compactly embeds into $L_p$. The key technical tool is a novel
bit-extraction technique which gives an optimal encoding of sparse vectors.
This enables us to obtain sharp upper bounds in the non-linear regime where $p
> q$. We also provide a novel method for deriving $L_p$-approximation lower
bounds based upon VC-dimension when $p < \infty$. Our results show that very
deep ReLU networks significantly outperform classical methods of approximation
in terms of the number of parameters, but that this comes at the cost of
parameters which are not encodable.


---

## 53. [AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval](https://arxiv.org/abs/2311.14084) <a name="link53"></a>
**ArXiv ID:** 2311.14084
**Authors:** Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, Xueqi Cheng

**Abstract:** With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon has elevated the issue of source bias in text retrieval for web
searches. Specifically, neural retrieval models tend to rank generated texts
higher than human-written texts. In this paper, we extend the study of this
bias to cross-modal retrieval. Firstly, we successfully construct a suitable
benchmark to explore the existence of the bias. Subsequent extensive
experiments on this benchmark reveal that AI-generated images introduce an
invisible relevance bias to text-image retrieval models. Specifically, our
experiments show that text-image retrieval models tend to rank the AI-generated
images higher than the real images, even though the AI-generated images do not
exhibit more visually relevant features to the query than real images. This
invisible relevance bias is prevalent across retrieval models with varying
training data and architectures. Furthermore, our subsequent exploration
reveals that the inclusion of AI-generated images in the training data of the
retrieval models exacerbates the invisible relevance bias. The above phenomenon
triggers a vicious cycle, which makes the invisible relevance bias become more
and more serious. To elucidate the potential causes of invisible relevance and
address the aforementioned issues, we introduce an effective training method
aimed at alleviating the invisible relevance bias. Subsequently, we apply our
proposed debiasing method to retroactively identify the causes of invisible
relevance, revealing that the AI-generated images induce the image encoder to
embed additional information into their representation. This information
exhibits a certain consistency across generated images with different semantics
and can make the retriever estimate a higher relevance score.


---

## 54. [Towards Transfer Learning for Large-Scale Image Classification Using Annealing-based Quantum Boltzmann Machines](https://arxiv.org/abs/2311.15966) <a name="link54"></a>
**ArXiv ID:** 2311.15966
**Authors:** Daniëlle Schuman, Leo Sünkel, Philipp Altmann, Jonas Stein, Christoph Roch, Thomas Gabor, Claudia Linnhoff-Popien

**Abstract:** Quantum Transfer Learning (QTL) recently gained popularity as a hybrid
quantum-classical approach for image classification tasks by efficiently
combining the feature extraction capabilities of large Convolutional Neural
Networks with the potential benefits of Quantum Machine Learning (QML).
Existing approaches, however, only utilize gate-based Variational Quantum
Circuits for the quantum part of these procedures. In this work we present an
approach to employ Quantum Annealing (QA) in QTL-based image classification.
Specifically, we propose using annealing-based Quantum Boltzmann Machines as
part of a hybrid quantum-classical pipeline to learn the classification of
real-world, large-scale data such as medical images through supervised
training. We demonstrate our approach by applying it to the three-class
COVID-CT-MD dataset, a collection of lung Computed Tomography (CT) scan slices.
Using Simulated Annealing as a stand-in for actual QA, we compare our method to
classical transfer learning, using a neural network of the same order of
magnitude, to display its improved classification performance. We find that our
approach consistently outperforms its classical baseline in terms of test
accuracy and AUC-ROC-Score and needs less training epochs to do this.


---

## 55. [From Pixels to Titles: Video Game Identification by Screenshots using Convolutional Neural Networks](https://arxiv.org/abs/2311.15963) <a name="link55"></a>
**ArXiv ID:** 2311.15963
**Authors:** Fabricio Breve

**Abstract:** This paper investigates video game identification through single screenshots,
utilizing five convolutional neural network (CNN) architectures (MobileNet,
DenseNet, EfficientNetB0, EfficientNetB2, and EfficientNetB3) across 22 home
console systems, spanning from Atari 2600 to PlayStation 5. Confirming the
hypothesis, CNNs autonomously extract image features, enabling the
identification of game titles from screenshots without additional features.
Using ImageNet pre-trained weights, EfficientNetB3 achieves the highest average
accuracy (74.51%), while DenseNet169 excels in 14 of the 22 systems. Employing
alternative initial weights from another screenshots dataset boosts accuracy
for EfficientNetB2 and EfficientNetB3, with the latter reaching a peak accuracy
of 76.36% and demonstrating reduced convergence epochs from 23.7 to 20.5 on
average. Overall, the combination of optimal architecture and weights attains
77.67% accuracy, primarily led by EfficientNetB3 in 19 systems. These findings
underscore the efficacy of CNNs in video game identification through
screenshots.


---

## 56. [A Social-aware Gaussian Pre-trained Model for Effective Cold-start Recommendation](https://arxiv.org/abs/2311.15790) <a name="link56"></a>
**ArXiv ID:** 2311.15790
**Authors:** Siwei Liu, Xi Wang, Craig Macdonald, Iadh Ounis

**Abstract:** The use of pre-training is an emerging technique to enhance a neural model's
performance, which has been shown to be effective for many neural language
models such as BERT. This technique has also been used to enhance the
performance of recommender systems. In such recommender systems, pre-training
models are used to learn a better initialisation for both users and items.
However, recent existing pre-trained recommender systems tend to only
incorporate the user interaction data at the pre-training stage, making it
difficult to deliver good recommendations, especially when the interaction data
is sparse. To alleviate this common data sparsity issue, we propose to
pre-train the recommendation model not only with the interaction data but also
with other available information such as the social relations among users,
thereby providing the recommender system with a better initialisation compared
with solely relying on the user interaction data. We propose a novel
recommendation model, the Social-aware Gaussian Pre-trained model (SGP), which
encodes the user social relations and interaction data at the pre-training
stage in a Graph Neural Network (GNN). Afterwards, in the subsequent
fine-tuning stage, our SGP model adopts a Gaussian Mixture Model (GMM) to
factorise these pre-trained embeddings for further training, thereby benefiting
the cold-start users from these pre-built social relations. Our extensive
experiments on three public datasets show that, in comparison to 16 competitive
baselines, our SGP model significantly outperforms the best baseline by upto
7.7% in terms of NDCG@10. In addition, we show that SGP permits to effectively
alleviate the cold-start problem, especially when users newly register to the
system through their friends' suggestions.


---

## 57. [Transformer-QEC: Quantum Error Correction Code Decoding with Transferable Transformers](https://arxiv.org/abs/2311.16082) <a name="link57"></a>
**ArXiv ID:** 2311.16082
**Authors:** Hanrui Wang, Pengyu Liu, Kevin Shao, Dantong Li, Jiaqi Gu, David Z. Pan, Yongshan Ding, Song Han

**Abstract:** Quantum computing has the potential to solve problems that are intractable
for classical systems, yet the high error rates in contemporary quantum devices
often exceed tolerable limits for useful algorithm execution. Quantum Error
Correction (QEC) mitigates this by employing redundancy, distributing quantum
information across multiple data qubits and utilizing syndrome qubits to
monitor their states for errors. The syndromes are subsequently interpreted by
a decoding algorithm to identify and correct errors in the data qubits. This
task is complex due to the multiplicity of error sources affecting both data
and syndrome qubits as well as syndrome extraction operations. Additionally,
identical syndromes can emanate from different error sources, necessitating a
decoding algorithm that evaluates syndromes collectively. Although machine
learning (ML) decoders such as multi-layer perceptrons (MLPs) and convolutional
neural networks (CNNs) have been proposed, they often focus on local syndrome
regions and require retraining when adjusting for different code distances. We
introduce a transformer-based QEC decoder which employs self-attention to
achieve a global receptive field across all input syndromes. It incorporates a
mixed loss training approach, combining both local physical error and global
parity label losses. Moreover, the transformer architecture's inherent
adaptability to variable-length inputs allows for efficient transfer learning,
enabling the decoder to adapt to varying code distances without retraining.
  Evaluation on six code distances and ten different error configurations
demonstrates that our model consistently outperforms non-ML decoders, such as
Union Find (UF) and Minimum Weight Perfect Matching (MWPM), and other ML
decoders, thereby achieving best logical error rates. Moreover, the transfer
learning can save over 10x of training cost.


---

## 58. [FLASC: A Flare-Sensitive Clustering Algorithm: Extending HDBSCAN* for Detecting Branches in Clusters](https://arxiv.org/abs/2311.15887) <a name="link58"></a>
**ArXiv ID:** 2311.15887
**Authors:** D. M. Bot, J. Peeters, J. Liesenborgs, J. Aerts

**Abstract:** We present FLASC, an algorithm for flare-sensitive clustering. Our algorithm
builds upon HDBSCAN* -- which provides high-quality density-based clustering
performance -- through a post-processing step that differentiates branches
within the detected clusters' manifold, adding a type of pattern that can be
discovered. Two variants of the algorithm are presented, which trade
computational cost for noise robustness. We show that both variants scale
similarly to HDBSCAN* in terms of computational cost and provide stable outputs
using synthetic data sets, resulting in an efficient flare-sensitive clustering
algorithm. In addition, we demonstrate the algorithm's benefit in data
exploration over HDBSCAN* clustering on two real-world data sets.


---

## 59. [Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation](https://arxiv.org/abs/2311.14353) <a name="link59"></a>
**ArXiv ID:** 2311.14353
**Authors:** Yasumasa Kano, Katsuhito Sudoh, Satoshi Nakamura

**Abstract:** Simultaneous translation is a task in which the translation begins before the
end of an input speech segment. Its evaluation should be conducted based on
latency in addition to quality, and for users, the smallest possible amount of
latency is preferable. Most existing metrics measure latency based on the start
timings of partial translations and ignore their duration. This means such
metrics do not penalize the latency caused by long translation output, which
delays the comprehension of users and subsequent translations. In this work, we
propose a novel latency evaluation metric for simultaneous translation called
\emph{Average Token Delay} (ATD) that focuses on the duration of partial
translations. We demonstrate its effectiveness through analyses simulating
user-side latency based on Ear-Voice Span (EVS). In our experiment, ATD had the
highest correlation with EVS among baseline latency metrics under most
conditions.


---

## 60. [Nodal Hydraulic Head Estimation through Unscented Kalman Filter for Data-driven Leak Localization in Water Networks](https://arxiv.org/abs/2311.15875) <a name="link60"></a>
**ArXiv ID:** 2311.15875
**Authors:** Luis Romero-Ben, Paul Irofti, Florin Stoican, Vicenç Puig

**Abstract:** In this paper, we present a nodal hydraulic head estimation methodology for
water distribution networks (WDN) based on an Unscented Kalman Filter (UKF)
scheme with application to leak localization. The UKF refines an initial
estimation of the hydraulic state by considering the prediction model, as well
as available pressure and demand measurements. To this end, it provides
customized prediction and data assimilation steps. Additionally, the method is
enhanced by dynamically updating the prediction function weight matrices.
Performance testing on the Modena benchmark under realistic conditions
demonstrates the method's effectiveness in enhancing state estimation and
data-driven leak localization.


---

## 61. [Online Estimation and Optimization of Utility-Based Shortfall Risk](https://arxiv.org/abs/2111.08805) <a name="link61"></a>
**ArXiv ID:** 2111.08805
**Authors:** Vishwajit Hegde, Arvind S. Menon, L. A. Prashanth, Krishna Jagannathan

**Abstract:** Utility-Based Shortfall Risk (UBSR) is a risk metric that is increasingly
popular in financial applications, owing to certain desirable properties that
it enjoys. We consider the problem of estimating UBSR in a recursive setting,
where samples from the underlying loss distribution are available
one-at-a-time. We cast the UBSR estimation problem as a root finding problem,
and propose stochastic approximation-based estimations schemes. We derive
non-asymptotic bounds on the estimation error in the number of samples. We also
consider the problem of UBSR optimization within a parameterized class of
random variables. We propose a stochastic gradient descent based algorithm for
UBSR optimization, and derive non-asymptotic bounds on its convergence.


---

## 62. [A new fuzzy multi-attribute group decision-making method based on TOPSIS and optimization models](https://arxiv.org/abs/2311.15933) <a name="link62"></a>
**ArXiv ID:** 2311.15933
**Authors:** Qixiao Hu, Shiquan Zhang, Chaolang Hu, Yuetong Liu

**Abstract:** In this paper, a new method based on TOPSIS and optimization models is
proposed for multi-attribute group decision-making in the environment of
interval-valued intuitionistic fuzzy sets.Firstly, by minimizing the sum of
differences between individual evaluations and the overallconsistent
evaluations of all experts, a new optimization model is established for
determining expert weights. Secondly, based on TOPSIS method, the improved
closeness index for evaluating each alternative is obtained. Finally, the
attribute weight is determined by establishing an optimization model with the
goal of maximizing the closeness of each alternative, and it is brought into
the closeness index so that the alternatives can be ranked. Combining all these
together, the complete fuzzy multi-attribute group decision-making algorithm is
formulated, which can give full play to the advantages of subjective and
objective weighting methods. In the end, the feasibility and effectiveness of
the provided method are verified by a real case study.


---

## 63. [Improved Data Generation for Enhanced Asset Allocation: A Synthetic Dataset Approach for the Fixed Income Universe](https://arxiv.org/abs/2311.16004) <a name="link63"></a>
**ArXiv ID:** 2311.16004
**Authors:** Szymon Kubiak, Tillman Weyde, Oleksandr Galkin, Dan Philps, Ram Gopal

**Abstract:** We present a novel process for generating synthetic datasets tailored to
assess asset allocation methods and construct portfolios within the fixed
income universe. Our approach begins by enhancing the CorrGAN model to generate
synthetic correlation matrices. Subsequently, we propose an Encoder-Decoder
model that samples additional data conditioned on a given correlation matrix.
The resulting synthetic dataset facilitates in-depth analyses of asset
allocation methods across diverse asset universes. Additionally, we provide a
case study that exemplifies the use of the synthetic dataset to improve
portfolios constructed within a simulation-based asset allocation process.


---

## 64. [RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization](https://arxiv.org/abs/2311.15876) <a name="link64"></a>
**ArXiv ID:** 2311.15876
**Authors:** Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Jin Sung Kim, Yong Bae Kim, Jong Chul Ye

**Abstract:** Recent advancements in Artificial Intelligence (AI) have profoundly
influenced medical fields, by providing tools to reduce clinical workloads.
However, most AI models are constrained to execute uni-modal tasks, in stark
contrast to the comprehensive approaches utilized by medical professionals. To
address this, here we present RO-LLaMA, a versatile generalist large language
model (LLM) tailored for the field of radiation oncology. This model seamlessly
covers a wide range of the workflow of radiation oncologists, adept at various
tasks such as clinical report summarization, radiation therapy plan suggestion,
and plan-guided therapy target volume segmentation. In particular, to maximize
the end-to-end performance, we further present a novel Consistency Embedding
Fine-Tuning (CEFTune) technique, which boosts LLM's robustness to additional
errors at the intermediates while preserving the capability of handling clean
inputs, and creatively transform this concept into LLM-driven segmentation
framework as Consistency Embedding Segmentation (CESEG). Experimental results
on multi-centre cohort sets demonstrate our proposed RO-LLaMA's promising
performance for diverse tasks with generalization capabilities.


---

## 65. [YUAN 2.0: A Large Language Model with Localized Filtering-based Attention](https://arxiv.org/abs/2311.15786) <a name="link65"></a>
**ArXiv ID:** 2311.15786
**Authors:** Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang

**Abstract:** In this work, the Localized Filtering-based Attention (LFA) is introduced to
incorporate prior knowledge of local dependencies of natural language into
Attention. Based on LFA, we develop and release Yuan 2.0, a large language
model with parameters ranging from 2.1 billion to 102.6 billion. A data
filtering and generation method is presented to build pretraining and
fine-tuning dataset in high quality. A distributed training method with
non-uniform pipeline parallel, data parallel, and optimizer parallel is
proposed, which greatly reduces the bandwidth requirements of intra-node
communication, and achieves good performance in large-scale distributed
training. Yuan 2.0 models display impressive ability in code generation, math
problem-solving, and chat compared with existing models. The latest version of
YUAN 2.0, including model weights and source code, is accessible at Github.


---

## 66. [Should We Learn Most Likely Functions or Parameters?](https://arxiv.org/abs/2311.15990) <a name="link66"></a>
**ArXiv ID:** 2311.15990
**Authors:** Shikai Qiu, Tim G. J. Rudner, Sanyam Kapoor, Andrew Gordon Wilson

**Abstract:** Standard regularized training procedures correspond to maximizing a posterior
distribution over parameters, known as maximum a posteriori (MAP) estimation.
However, model parameters are of interest only insomuch as they combine with
the functional form of a model to provide a function that can make good
predictions. Moreover, the most likely parameters under the parameter posterior
do not generally correspond to the most likely function induced by the
parameter posterior. In fact, we can re-parametrize a model such that any
setting of parameters can maximize the parameter posterior. As an alternative,
we investigate the benefits and drawbacks of directly estimating the most
likely function implied by the model and the data. We show that this procedure
leads to pathological solutions when using neural networks and prove conditions
under which the procedure is well-behaved, as well as a scalable approximation.
Under these conditions, we find that function-space MAP estimation can lead to
flatter minima, better generalization, and improved robustness to overfitting.


---

## 67. [A deep reinforcement learning model for predictive maintenance planning of road assets: Integrating LCA and LCCA](https://arxiv.org/abs/2112.12589) <a name="link67"></a>
**ArXiv ID:** 2112.12589
**Authors:** Moein Latifi, Fateme Golivand Darvishvand, Omid Khandel, Mobin Latifi Nowsoud

**Abstract:** Road maintenance planning is an integral part of road asset management. One
of the main challenges in Maintenance and Rehabilitation (M&R) practices is to
determine maintenance type and timing. This research proposes a framework using
Reinforcement Learning (RL) based on the Long Term Pavement Performance (LTPP)
database to determine the type and timing of M&R practices. A predictive DNN
model is first developed in the proposed algorithm, which serves as the
Environment for the RL algorithm. For the Policy estimation of the RL model,
both DQN and PPO models are developed. However, PPO has been selected in the
end due to better convergence and higher sample efficiency. Indicators used in
this study are International Roughness Index (IRI) and Rutting Depth (RD).
Initially, we considered Cracking Metric (CM) as the third indicator, but it
was then excluded due to the much fewer data compared to other indicators,
which resulted in lower accuracy of the results. Furthermore, in
cost-effectiveness calculation (reward), we considered both the economic and
environmental impacts of M&R treatments. Costs and environmental impacts have
been evaluated with paLATE 2.0 software. Our method is tested on a hypothetical
case study of a six-lane highway with 23 kilometers length located in Texas,
which has a warm and wet climate. The results propose a 20-year M&R plan in
which road condition remains in an excellent condition range. Because the early
state of the road is at a good level of service, there is no need for heavy
maintenance practices in the first years. Later, after heavy M&R actions, there
are several 1-2 years of no need for treatments. All of these show that the
proposed plan has a logical result. Decision-makers and transportation agencies
can use this scheme to conduct better maintenance practices that can prevent
budget waste and, at the same time, minimize the environmental impacts.


---

## 68. [Soil Organic Carbon Estimation from Climate-related Features with Graph Neural Network](https://arxiv.org/abs/2311.15979) <a name="link68"></a>
**ArXiv ID:** 2311.15979
**Authors:** Weiying Zhao, Natalia Efremova

**Abstract:** Soil organic carbon (SOC) plays a pivotal role in the global carbon cycle,
impacting climate dynamics and necessitating accurate estimation for
sustainable land and agricultural management. While traditional methods of SOC
estimation face resolution and accuracy challenges, recent technological
solutions harness remote sensing, machine learning, and high-resolution
satellite mapping. Graph Neural Networks (GNNs), especially when integrated
with positional encoders, can capture complex relationships between soil and
climate. Using the LUCAS database, this study compared four GNN operators in
the positional encoder framework. Results revealed that the PESAGE and
PETransformer models outperformed others in SOC estimation, indicating their
potential in capturing the complex relationship between SOC and climate
features. Our findings confirm the feasibility of applications of GNN
architectures in SOC prediction, establishing a framework for future
explorations of this topic with more advanced GNN models.


---

## 69. [Towards Adaptive RF Fingerprint-based Authentication of IIoT devices](https://arxiv.org/abs/2311.15888) <a name="link69"></a>
**ArXiv ID:** 2311.15888
**Authors:** Emmanuel Lomba, Ricardo Severino, Ana Fernández Vilas

**Abstract:** As IoT technologies mature, they are increasingly finding their way into more
sensitive domains, such as Medical and Industrial IoT, in which safety and
cyber-security are of great importance. While the number of deployed IoT
devices continues to increase exponentially, they still present severe
cyber-security vulnerabilities. Effective authentication is paramount to
support trustworthy IIoT communications, however, current solutions focus on
upper-layer identity verification or key-based cryptography which are often
inadequate to the heterogeneous IIoT environment. In this work, we present a
first step towards achieving powerful and flexible IIoT device authentication,
by leveraging AI adaptive Radio Frequency Fingerprinting technique selection
and tuning, at the PHY layer for highly accurate device authentication over
challenging RF environments.


---

## 70. [Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines](https://arxiv.org/abs/2311.15960) <a name="link70"></a>
**ArXiv ID:** 2311.15960
**Authors:** Yu-An Lin, Chen-Tao Lee, Guan-Ting Liu, Pu-Jen Cheng, Shao-Hua Sun

**Abstract:** Deep reinforcement learning excels in various domains but lacks
generalizability and interoperability. Programmatic RL methods (Trivedi et al.,
2021; Liu et al., 2023) reformulate solving RL tasks as synthesizing
interpretable programs that can be executed in the environments. Despite
encouraging results, these methods are limited to short-horizon tasks. On the
other hand, representing RL policies using state machines (Inala et al., 2020)
can inductively generalize to long-horizon tasks; however, it struggles to
scale up to acquire diverse and complex behaviors. This work proposes Program
Machine Policies (POMPs), which bridge the advantages of programmatic RL and
state machine policies, allowing for the representation of complex behaviors
and the address of long-term tasks. Specifically, we introduce a method that
can retrieve a set of effective, diverse, compatible programs. Then, we use
these programs as modes of a state machine and learn a transition function to
transition among mode programs, allowing for capturing long-horizon repetitive
behaviors. Our proposed framework outperforms programmatic RL and deep RL
baselines on various tasks and demonstrates the ability to generalize to even
longer horizons without any fine-tuning inductively. Ablation studies justify
the effectiveness of our proposed search algorithm for retrieving a set of
programs as modes.


---

## 71. [Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework](https://arxiv.org/abs/2311.15993) <a name="link71"></a>
**ArXiv ID:** 2311.15993
**Authors:** Shaobo Wang, Xiangdong Zhang, Junchi Yan

**Abstract:** Batch Normalization (BN) has become an essential technique in contemporary
neural network design, enhancing training stability. Specifically, BN employs
centering and scaling operations to standardize features along the batch
dimension and uses an affine transformation to recover features. Although
standard BN has shown its capability to improve deep neural network training
and convergence, it still exhibits inherent limitations in certain cases. Most
existing techniques that enhance BN consider a single or a few aspects of BN.
In this paper, we first identify problems with BN from a feature perspective
and explore that feature condensation exists in the learning when employing BN,
which negatively affects testing performance. To tackle this problem, we
propose a two-stage unified framework called Unified Batch Normalization (UBN).
In the first stage, we utilize a simple feature condensation threshold to
alleviate the feature condensation, which hinders inappropriate statistic
updates in normalization. In the second stage, we unify various normalization
variants to boost each component of BN. Our experimental results reveal that
UBN significantly enhances performance across different visual backbones and
notably expedites network training convergence, particularly in early training
stages. Notably, our method improved about 3% in top-1 accuracy on ImageNet
classification with large batch sizes, showing the effectiveness of our
approach in real-world scenarios.


---

## 72. [RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment](https://arxiv.org/abs/2305.19599) <a name="link72"></a>
**ArXiv ID:** 2305.19599
**Authors:** Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Shengcai Liao, Xiaodan Liang

**Abstract:** Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from textual descriptions.
However, these approaches have faced challenges in precisely aligning the
generated visual content with the textual concepts described in the prompts. In
this paper, we propose a two-stage coarse-to-fine semantic re-alignment method,
named RealignDiff, aimed at improving the alignment between text and images in
text-to-image diffusion models. In the coarse semantic re-alignment phase, a
novel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the
semantic discrepancy between the generated image caption and the given text
prompt. Subsequently, the fine semantic re-alignment stage employs a local
dense caption generation module and a re-weighting attention modulation module
to refine the previously generated images from a local semantic view.
Experimental results on the MS-COCO benchmark demonstrate that the proposed
two-stage coarse-to-fine semantic re-alignment method outperforms other
baseline re-alignment techniques by a substantial margin in both visual quality
and semantic similarity with the input prompt.


---

## 73. [Machine learning-based decentralized TDMA for VLC IoT networks](https://arxiv.org/abs/2311.14078) <a name="link73"></a>
**ArXiv ID:** 2311.14078
**Authors:** Armin Makvandi, Yousef Seifi Kavian

**Abstract:** In this paper, a machine learning-based decentralized time division multiple
access (TDMA) algorithm for visible light communication (VLC) Internet of
Things (IoT) networks is proposed. The proposed algorithm is based on
Q-learning, a reinforcement learning algorithm. This paper considers a
decentralized condition in which there is no coordinator node for sending
synchronization frames and assigning transmission time slots to other nodes.
The proposed algorithm uses a decentralized manner for synchronization, and
each node uses the Q-learning algorithm to find the optimal transmission time
slot for sending data without collisions. The proposed algorithm is implemented
on a VLC hardware system, which had been designed and implemented in our
laboratory. Average reward, convergence time, goodput, average delay, and data
packet size are evaluated parameters. The results show that the proposed
algorithm converges quickly and provides collision-free decentralized TDMA for
the network. The proposed algorithm is compared with carrier-sense multiple
access with collision avoidance (CSMA/CA) algorithm as a potential selection
for decentralized VLC IoT networks. The results show that the proposed
algorithm provides up to 61% more goodput and up to 49% less average delay than
CSMA/CA.


---

## 74. [Tell2Design: A Dataset for Language-Guided Floor Plan Generation](https://arxiv.org/abs/2311.15941) <a name="link74"></a>
**ArXiv ID:** 2311.15941
**Authors:** Sicong Leng, Yang Zhou, Mohammed Haroon Dupty, Wee Sun Lee, Sam Conrad Joyce, Wei Lu

**Abstract:** We consider the task of generating designs directly from natural language
descriptions, and consider floor plan generation as the initial research area.
Language conditional generative models have recently been very successful in
generating high-quality artistic images. However, designs must satisfy
different constraints that are not present in generating artistic images,
particularly spatial and relational constraints. We make multiple contributions
to initiate research on this task. First, we introduce a novel dataset,
\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs
associated with natural language instructions. Second, we propose a
Sequence-to-Sequence model that can serve as a strong baseline for future
research. Third, we benchmark this task with several text-conditional image
generation models. We conclude by conducting human evaluations on the generated
samples and providing an analysis of human performance. We hope our
contributions will propel the research on language-guided design generation
forward.


---

## 75. [XLB: Distributed Multi-GPU Lattice Boltzmann Simulation Framework for Differentiable Scientific Machine Learning](https://arxiv.org/abs/2311.16080) <a name="link75"></a>
**ArXiv ID:** 2311.16080
**Authors:** Mohammadmehdi Ataei, Hesam Salehipour

**Abstract:** The lattice Boltzmann method (LBM) has emerged as a prominent technique for
solving fluid dynamics problems due to its algorithmic potential for
computational scalability. We introduce XLB framework, a Python-based
differentiable LBM library which harnesses the capabilities of the JAX
framework. The architecture of XLB is predicated upon ensuring accessibility,
extensibility, and computational performance, enabling scaling effectively
across CPU, multi-GPU, and distributed multi-GPU systems. The framework can be
readily augmented with novel boundary conditions, collision models, or
simulation capabilities. XLB offers the unique advantage of integration with
JAX's extensive machine learning echosystem, and the ability to utilize
automatic differentiation for tackling physics-based machine learning,
optimization, and inverse problems. XLB has been successfully scaled to handle
simulations with billions of cells, achieving giga-scale lattice updates per
second. XLB is released under the permissive Apache-2.0 license and is
available on GitHub at https://github.com/Autodesk/XLB.


---

## 76. [DUnE: Dataset for Unified Editing](https://arxiv.org/abs/2311.16087) <a name="link76"></a>
**ArXiv ID:** 2311.16087
**Authors:** Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, Derry Wijaya

**Abstract:** Even the most advanced language models remain susceptible to errors
necessitating to modify these models without initiating a comprehensive
retraining process. Model editing refers to the modification of a model's
knowledge or representations in a manner that produces the desired outcomes.
Prior research primarily centered around editing factual data e.g. "Messi plays
for Inter Miami" confining the definition of an edit to a knowledge triplet
i.e. (subject, object, relation). However, as the applications of language
models expand, so do the diverse ways in which we wish to edit and refine their
outputs. In this study, we broaden the scope of the editing problem to include
an array of editing cases such as debiasing and rectifying reasoning errors and
define an edit as any natural language expression that solicits a change in the
model's outputs. We are introducing DUnE-an editing benchmark where edits are
natural language sentences and propose that DUnE presents a challenging yet
relevant task. To substantiate this claim, we conduct an extensive series of
experiments testing various editing approaches to address DUnE, demonstrating
their respective strengths and weaknesses. We show that retrieval-augmented
language modeling can outperform specialized editing techniques and neither set
of approaches has fully solved the generalized editing problem covered by our
benchmark.


---

## 77. [Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation](https://arxiv.org/abs/2203.05400) <a name="link77"></a>
**ArXiv ID:** 2203.05400
**Authors:** Toni Karvonen

**Abstract:** It is common to model a deterministic response function, such as the output
of a computer experiment, as a Gaussian process with a Mat\'ern covariance
kernel. The smoothness parameter of a Mat\'ern kernel determines many important
properties of the model in the large data limit, including the rate of
convergence of the conditional mean to the response function. We prove that the
maximum likelihood estimate of the smoothness parameter cannot asymptotically
undersmooth the truth when the data are obtained on a fixed bounded subset of
$\mathbb{R}^d$. That is, if the data-generating response function has Sobolev
smoothness $\nu_0 > d/2$, then the smoothness parameter estimate cannot be
asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we
show that maximum likelihood estimation recovers the true smoothness for a
class of compactly supported self-similar functions. For cross-validation we
prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be
sharp. The results are based on approximation theory in Sobolev spaces and some
general theorems that restrict the set of values that the parameter estimators
can take.


---

## 78. [Generative AI and US Intellectual Property Law](https://arxiv.org/abs/2311.16023) <a name="link78"></a>
**ArXiv ID:** 2311.16023
**Authors:** Cherie M Poland

**Abstract:** The rapidity with which generative AI has been adopted and advanced has
raised legal and ethical questions related to the impact on artists rights,
content production, data collection, privacy, accuracy of information, and
intellectual property rights. Recent administrative and case law challenges
have shown that generative AI software systems do not have independent
intellectual property rights in the content that they generate. It remains to
be seen whether human content creators can retain their intellectual property
rights against generative AI software, its developers, operators, and owners
for the misappropriation of the work of human creatives, given the metes and
bounds of existing law. Early signs from various courts are mixed as to whether
and to what degree the results generated by AI models meet the legal standards
of infringement under existing law.


---

## 79. [On Bringing Robots Home](https://arxiv.org/abs/2311.16098) <a name="link79"></a>
**ArXiv ID:** 2311.16098
**Authors:** Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, Lerrel Pinto

**Abstract:** Throughout history, we have successfully integrated various machines into our
homes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few
recent examples. However, these machines excel at performing only a single task
effectively. The concept of a "generalist machine" in homes - a domestic
assistant that can adapt and learn from our needs, all while remaining
cost-effective - has long been a goal in robotics that has been steadily
pursued for decades. In this work, we initiate a large-scale effort towards
this goal by introducing Dobb-E, an affordable yet versatile general-purpose
system for learning robotic manipulation within household settings. Dobb-E can
learn a new task with only five minutes of a user showing it how to do it,
thanks to a demonstration collection tool ("The Stick") we built out of cheap
parts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of
New York City, and train Home Pretrained Representations (HPR). Then, in a
novel home environment, with five minutes of demonstrations and fifteen minutes
of adapting the HPR model, we show that Dobb-E can reliably solve the task on
the Stretch, a mobile robot readily available on the market. Across roughly 30
days of experimentation in homes of New York City and surrounding areas, we
test our system in 10 homes, with a total of 109 tasks in different
environments, and finally achieve a success rate of 81%. Beyond success
percentages, our experiments reveal a plethora of unique challenges absent or
ignored in lab robotics. These range from effects of strong shadows, to
variable demonstration quality by non-expert users. With the hope of
accelerating research on home robots, and eventually seeing robot butlers in
every home, we open-source Dobb-E software stack and models, our data, and our
hardware designs at https://dobb-e.com


---

## 80. [CheapNET: Improving Light-weight speech enhancement network by projected loss function](https://arxiv.org/abs/2311.15959) <a name="link80"></a>
**ArXiv ID:** 2311.15959
**Authors:** Kaijun Tan, Benzhe Dai, Jiakui Li, Wenyu Mao

**Abstract:** Noise suppression and echo cancellation are critical in speech enhancement
and essential for smart devices and real-time communication. Deployed in voice
processing front-ends and edge devices, these algorithms must ensure efficient
real-time inference with low computational demands. Traditional edge-based
noise suppression often uses MSE-based amplitude spectrum mask training, but
this approach has limitations. We introduce a novel projection loss function,
diverging from MSE, to enhance noise suppression. This method uses projection
techniques to isolate key audio components from noise, significantly improving
model performance. For echo cancellation, the function enables direct
predictions on LAEC pre-processed outputs, substantially enhancing performance.
Our noise suppression model achieves near state-of-the-art results with only
3.1M parameters and 0.4GFlops/s computational load. Moreover, our echo
cancellation model outperforms replicated industry-leading models, introducing
a new perspective in speech enhancement.


---

## 81. [FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax](https://arxiv.org/abs/2311.15813) <a name="link81"></a>
**ArXiv ID:** 2311.15813
**Authors:** Yu Lu, Linchao Zhu, Hehe Fan, Yi Yang

**Abstract:** Text-to-video (T2V) generation is a rapidly growing research area that aims
to translate the scenes, objects, and actions within complex video text into a
sequence of coherent visual frames. We present FlowZero, a novel framework that
combines Large Language Models (LLMs) with image diffusion models to generate
temporally-coherent videos. FlowZero uses LLMs to understand complex
spatio-temporal dynamics from text, where LLMs can generate a comprehensive
dynamic scene syntax (DSS) containing scene descriptions, object layouts, and
background motion patterns. These elements in DSS are then used to guide the
image diffusion model for video generation with smooth object motions and
frame-to-frame coherence. Moreover, FlowZero incorporates an iterative
self-refinement process, enhancing the alignment between the spatio-temporal
layouts and the textual prompts for the videos. To enhance global coherence, we
propose enriching the initial noise of each frame with motion dynamics to
control the background movement and camera motion adaptively. By using
spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves
improvement in zero-shot video synthesis, generating coherent videos with vivid
motion.


---

## 82. [A Neural Framework for Generalized Causal Sensitivity Analysis](https://arxiv.org/abs/2311.16026) <a name="link82"></a>
**ArXiv ID:** 2311.16026
**Authors:** Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar

**Abstract:** Unobserved confounding is common in many applications, making causal
inference from observational data challenging. As a remedy, causal sensitivity
analysis is an important tool to draw causal conclusions under unobserved
confounding with mathematical guarantees. In this paper, we propose NeuralCSA,
a neural framework for generalized causal sensitivity analysis. Unlike previous
work, our framework is compatible with (i) a large class of sensitivity models,
including the marginal sensitivity model, f-sensitivity models, and Rosenbaum's
sensitivity model; (ii) different treatment types (i.e., binary and
continuous); and (iii) different causal queries, including (conditional)
average treatment effects and simultaneous effects on multiple outcomes. The
generality of \frameworkname is achieved by learning a latent distribution
shift that corresponds to a treatment intervention using two conditional
normalizing flows. We provide theoretical guarantees that NeuralCSA is able to
infer valid bounds on the causal query of interest and also demonstrate this
empirically using both simulated and real-world data.


---

## 83. [Sensitivity-Based Layer Insertion for Residual and Feedforward Neural Networks](https://arxiv.org/abs/2311.15995) <a name="link83"></a>
**ArXiv ID:** 2311.15995
**Authors:** Evelyn Herberg, Roland Herzog, Frederik Köhne, Leonie Kreis, Anton Schiela

**Abstract:** The training of neural networks requires tedious and often manual tuning of
the network architecture. We propose a systematic method to insert new layers
during the training process, which eliminates the need to choose a fixed
network size before training. Our technique borrows techniques from constrained
optimization and is based on first-order sensitivity information of the
objective with respect to the virtual parameters that additional layers, if
inserted, would offer. We consider fully connected feedforward networks with
selected activation functions as well as residual neural networks. In numerical
experiments, the proposed sensitivity-based layer insertion technique exhibits
improved training decay, compared to not inserting the layer. Furthermore, the
computational effort is reduced in comparison to inserting the layer from the
beginning. The code is available at
\url{https://github.com/LeonieKreis/layer_insertion_sensitivity_based}.


---

## 84. [Multi-Agent Reinforcement Learning for Power Control in Wireless Networks via Adaptive Graphs](https://arxiv.org/abs/2311.15858) <a name="link84"></a>
**ArXiv ID:** 2311.15858
**Authors:** Lorenzo Mario Amorosa, Marco Skocaj, Roberto Verdone, Deniz Gündüz

**Abstract:** The ever-increasing demand for high-quality and heterogeneous wireless
communication services has driven extensive research on dynamic optimization
strategies in wireless networks. Among several possible approaches, multi-agent
deep reinforcement learning (MADRL) has emerged as a promising method to
address a wide range of complex optimization problems like power control.
However, the seamless application of MADRL to a variety of network optimization
problems faces several challenges related to convergence. In this paper, we
present the use of graphs as communication-inducing structures among
distributed agents as an effective means to mitigate these challenges.
Specifically, we harness graph neural networks (GNNs) as neural architectures
for policy parameterization to introduce a relational inductive bias in the
collective decision-making process. Most importantly, we focus on modeling the
dynamic interactions among sets of neighboring agents through the introduction
of innovative methods for defining a graph-induced framework for integrated
communication and learning. Finally, the superior generalization capabilities
of the proposed methodology to larger networks and to networks with different
user categories is verified through simulations.


---

## 85. [Closing the ODE-SDE gap in score-based diffusion models through the Fokker-Planck equation](https://arxiv.org/abs/2311.15996) <a name="link85"></a>
**ArXiv ID:** 2311.15996
**Authors:** Teo Deveney, Jan Stanczuk, Lisa Maria Kreusser, Chris Budd, Carola-Bibiane Schönlieb

**Abstract:** Score-based diffusion models have emerged as one of the most promising
frameworks for deep generative modelling, due to their state-of-the art
performance in many generation tasks while relying on mathematical foundations
such as stochastic differential equations (SDEs) and ordinary differential
equations (ODEs). Empirically, it has been reported that ODE based samples are
inferior to SDE based samples. In this paper we rigorously describe the range
of dynamics and approximations that arise when training score-based diffusion
models, including the true SDE dynamics, the neural approximations, the various
approximate particle dynamics that result, as well as their associated
Fokker--Planck equations and the neural network approximations of these
Fokker--Planck equations. We systematically analyse the difference between the
ODE and SDE dynamics of score-based diffusion models, and link it to an
associated Fokker--Planck equation. We derive a theoretical upper bound on the
Wasserstein 2-distance between the ODE- and SDE-induced distributions in terms
of a Fokker--Planck residual. We also show numerically that conventional
score-based diffusion models can exhibit significant differences between ODE-
and SDE-induced distributions which we demonstrate using explicit comparisons.
Moreover, we show numerically that reducing the Fokker--Planck residual by
adding it as an additional regularisation term leads to closing the gap between
ODE- and SDE-induced distributions. Our experiments suggest that this
regularisation can improve the distribution generated by the ODE, however that
this can come at the cost of degraded SDE sample quality.


---

## 86. [MetaDefa: Meta-learning based on Domain Enhancement and Feature Alignment for Single Domain Generalization](https://arxiv.org/abs/2311.15906) <a name="link86"></a>
**ArXiv ID:** 2311.15906
**Authors:** Can Sun, Hao Zheng, Zhigang Hu, Liu Yang, Meiguang Zheng, Bo Xu

**Abstract:** The single domain generalization(SDG) based on meta-learning has emerged as
an effective technique for solving the domain-shift problem. However, the
inadequate match of data distribution between source and augmented domains and
difficult separation of domain-invariant features from domain-related features
make SDG model hard to achieve great generalization. Therefore, a novel
meta-learning method based on domain enhancement and feature alignment
(MetaDefa) is proposed to improve the model generalization performance. First,
the background substitution and visual corruptions techniques are used to
generate diverse and effective augmented domains. Then, the multi-channel
feature alignment module based on class activation maps and class agnostic
activation maps is designed to effectively extract adequate transferability
knowledge. In this module, domain-invariant features can be fully explored by
focusing on similar target regions between source and augmented domains feature
space and suppressing the feature representation of non-similar target regions.
Extensive experiments on two publicly available datasets show that MetaDefa has
significant generalization performance advantages in unknown multiple target
domains.


---

## 87. [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing](https://arxiv.org/abs/2309.16883) <a name="link87"></a>
**ArXiv ID:** 2309.16883
**Authors:** Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen

**Abstract:** Real-life applications of deep neural networks are hindered by their unsteady
predictions when faced with noisy inputs and adversarial attacks. The certified
radius is in this context a crucial indicator of the robustness of models.
However how to design an efficient classifier with a sufficient certified
radius? Randomized smoothing provides a promising framework by relying on noise
injection in inputs to obtain a smoothed and more robust classifier. In this
paper, we first show that the variance introduced by randomized smoothing
closely interacts with two other important properties of the classifier,
\textit{i.e.} its Lipschitz constant and margin. More precisely, our work
emphasizes the dual impact of the Lipschitz constant of the base classifier, on
both the smoothed classifier and the empirical variance. Moreover, to increase
the certified robust radius, we introduce a different simplex projection
technique for the base classifier to leverage the variance-margin trade-off
thanks to Bernstein's concentration inequality, along with an enhanced
Lipschitz bound. Experimental results show a significant improvement in
certified accuracy compared to current state-of-the-art methods. Our novel
certification procedure allows us to use pre-trained models that are used with
randomized smoothing, effectively improving the current certification radius in
a zero-shot manner.


---

## 88. [Stability-Informed Initialization of Neural Ordinary Differential Equations](https://arxiv.org/abs/2311.15890) <a name="link88"></a>
**ArXiv ID:** 2311.15890
**Authors:** Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk

**Abstract:** This paper addresses the training of Neural Ordinary Differential Equations
(neural ODEs), and in particular explores the interplay between numerical
integration techniques, stability regions, step size, and initialization
techniques. It is shown how the choice of integration technique implicitly
regularizes the learned model, and how the solver's corresponding stability
region affects training and prediction performance. From this analysis, a
stability-informed parameter initialization technique is introduced. The
effectiveness of the initialization method is displayed across several learning
benchmarks and industrial applications.


---

## 89. [Towards Responsible Governance of Biological Design Tools](https://arxiv.org/abs/2311.15936) <a name="link89"></a>
**ArXiv ID:** 2311.15936
**Authors:** Richard Moulange, Max Langenkamp, Tessa Alexanian, Samuel Curtis, Morgan Livingston

**Abstract:** Recent advancements in generative machine learning have enabled rapid
progress in biological design tools (BDTs) such as protein structure and
sequence prediction models. The unprecedented predictive accuracy and novel
design capabilities of BDTs present new and significant dual-use risks. For
example, their predictive accuracy allows biological agents, whether vaccines
or pathogens, to be developed more quickly, while the design capabilities could
be used to discover drugs or evade DNA screening techniques. Similar to other
dual-use AI systems, BDTs present a wicked problem: how can regulators uphold
public safety without stifling innovation? We highlight how current regulatory
proposals that are primarily tailored toward large language models may be less
effective for BDTs, which require fewer computational resources to train and
are often developed in an open-source manner. We propose a range of measures to
mitigate the risk that BDTs are misused, across the areas of responsible
development, risk assessment, transparency, access management, cybersecurity,
and investing in resilience. Implementing such measures will require close
coordination between developers and governments.


---

## 90. [RobustState: Boosting Fidelity of Quantum State Preparation via Noise-Aware Variational Training](https://arxiv.org/abs/2311.16035) <a name="link90"></a>
**ArXiv ID:** 2311.16035
**Authors:** Hanrui Wang, Yilian Liu, Pengyu Liu, Jiaqi Gu, Zirui Li, Zhiding Liang, Jinglei Cheng, Yongshan Ding, Xuehai Qian, Yiyu Shi, David Z. Pan, Frederic T. Chong, Song Han

**Abstract:** Quantum state preparation, a crucial subroutine in quantum computing,
involves generating a target quantum state from initialized qubits. Arbitrary
state preparation algorithms can be broadly categorized into arithmetic
decomposition (AD) and variational quantum state preparation (VQSP). AD employs
a predefined procedure to decompose the target state into a series of gates,
whereas VQSP iteratively tunes ansatz parameters to approximate target state.
VQSP is particularly apt for Noisy-Intermediate Scale Quantum (NISQ) machines
due to its shorter circuits. However, achieving noise-robust parameter
optimization still remains challenging.
  We present RobustState, a novel VQSP training methodology that combines high
robustness with high training efficiency. The core idea involves utilizing
measurement outcomes from real machines to perform back-propagation through
classical simulators, thus incorporating real quantum noise into gradient
calculations. RobustState serves as a versatile, plug-and-play technique
applicable for training parameters from scratch or fine-tuning existing
parameters to enhance fidelity on target machines. It is adaptable to various
ansatzes at both gate and pulse levels and can even benefit other variational
algorithms, such as variational unitary synthesis.
  Comprehensive evaluation of RobustState on state preparation tasks for 4
distinct quantum algorithms using 10 real quantum machines demonstrates a
coherent error reduction of up to 7.1 $\times$ and state fidelity improvement
of up to 96\% and 81\% for 4-Q and 5-Q states, respectively. On average,
RobustState improves fidelity by 50\% and 72\% for 4-Q and 5-Q states compared
to baseline approaches.


---

## 91. [Dimensionality Reduction and Wasserstein Stability for Kernel Regression](https://arxiv.org/abs/2203.09347) <a name="link91"></a>
**ArXiv ID:** 2203.09347
**Authors:** Stephan Eckstein, Armin Iske, Mathias Trabs

**Abstract:** In a high-dimensional regression framework, we study consequences of the
naive two-step procedure where first the dimension of the input variables is
reduced and second, the reduced input variables are used to predict the output
variable with kernel regression. In order to analyze the resulting regression
errors, a novel stability result for kernel regression with respect to the
Wasserstein distance is derived. This allows us to bound errors that occur when
perturbed input data is used to fit the regression function. We apply the
general stability result to principal component analysis (PCA). Exploiting
known estimates from the literature on both principal component analysis and
kernel regression, we deduce convergence rates for the two-step procedure. The
latter turns out to be particularly useful in a semi-supervised setting.


---

## 92. [Efficient Perception, Planning, and Control Algorithms for Vision-Based Automated Vehicles](https://arxiv.org/abs/2209.07042) <a name="link92"></a>
**ArXiv ID:** 2209.07042
**Authors:** Der-Hau Lee

**Abstract:** Autonomous vehicles have limited computational resources; hence, their
control systems must be efficient. The cost and size of sensors have limited
the development of self-driving cars. To overcome these restrictions, this
study proposes an efficient framework for the operation of vision-based
automatic vehicles; the framework requires only a monocular camera and a few
inexpensive radars. The proposed algorithm comprises a multi-task UNet (MTUNet)
network for extracting image features and constrained iterative linear
quadratic regulator (CILQR) and vision predictive control (VPC) modules for
rapid motion planning and control. MTUNet is designed to simultaneously solve
lane line segmentation, the ego vehicle's heading angle regression, road type
classification, and traffic object detection tasks at approximately 40 FPS
(frames per second) for 228 x 228 pixel RGB input images. The CILQR controllers
then use the MTUNet outputs and radar data as inputs to produce driving
commands for lateral and longitudinal vehicle guidance within only 1 ms. In
particular, the VPC algorithm is included to reduce steering command latency to
below actuator latency to prevent self-driving vehicle performance degradation
during tight turns. The VPC algorithm uses road curvature data from MTUNet to
estimate the correction of the current steering angle at a look-ahead point to
adjust the turning amount. Including the VPC algorithm in a VPC-CILQR
controller on curvy roads leads to higher performance than CILQR alone. Our
experiments demonstrate that the proposed autonomous driving system, which does
not require high-definition maps, could be applied in current autonomous
vehicles.


---

## 93. [Neuradicon: operational representation learning of neuroimaging reports](https://arxiv.org/abs/2107.10021) <a name="link93"></a>
**ArXiv ID:** 2107.10021
**Authors:** Henry Watkins, Robert Gray, Adam Julius, Yee-Haur Mah, Walter H. L. Pinaya, Paul Wright, Ashwani Jha, Holger Engleitner, Jorge Cardoso, Sebastien Ourselin, Geraint Rees, Rolf Jaeger, Parashkev Nachev

**Abstract:** Radiological reports typically summarize the content and interpretation of
imaging studies in unstructured form that precludes quantitative analysis. This
limits the monitoring of radiological services to throughput undifferentiated
by content, impeding specific, targeted operational optimization. Here we
present Neuradicon, a natural language processing (NLP) framework for
quantitative analysis of neuroradiological reports. Our framework is a hybrid
of rule-based and artificial intelligence models to represent neurological
reports in succinct, quantitative form optimally suited to operational
guidance. We demonstrate the application of Neuradicon to operational
phenotyping of a corpus of 336,569 reports, and report excellent
generalizability across time and two independent healthcare institutions.


---

## 94. [MEDITRON-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079) <a name="link94"></a>
**ArXiv ID:** 2311.16079
**Authors:** Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut

**Abstract:** Large language models (LLMs) can potentially democratize access to medical
knowledge. While many efforts have been made to harness and improve LLMs'
medical knowledge and reasoning capacities, the resulting models are either
closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),
which restricts their abilities. In this work, we improve access to large-scale
medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B
parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through
our adaptation of Nvidia's Megatron-LM distributed trainer), and extends
pretraining on a comprehensively curated medical corpus, including selected
PubMed articles, abstracts, and internationally-recognized medical guidelines.
Evaluations using four major medical benchmarks show significant performance
gains over several state-of-the-art baselines before and after task-specific
finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the
best public baseline in its parameter class and 3% over the strongest baseline
we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B
outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of
Med-PaLM-2. We release our code for curating the medical pretraining corpus and
the MEDITRON model weights to drive open-source development of more capable
medical LLMs.


---

## 95. [Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks](https://arxiv.org/abs/2311.11913) <a name="link95"></a>
**ArXiv ID:** 2311.11913
**Authors:** Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum

**Abstract:** The ability to construct a realistic simulator of financial exchanges,
including reproducing the dynamics of the limit order book, can give insight
into many counterfactual scenarios, such as a flash crash, a margin call, or
changes in macroeconomic outlook. In recent years, agent-based models have been
developed that reproduce many features of an exchange, as summarised by a set
of stylised facts and statistics. However, the ability to calibrate simulators
to a specific period of trading remains an open challenge. In this work, we
develop a novel approach to the calibration of market simulators by leveraging
recent advances in deep learning, specifically using neural density estimators
and embedding networks. We demonstrate that our approach is able to correctly
identify high probability parameter sets, both when applied to synthetic and
historical data, and without reliance on manually selected or weighted
ensembles of stylised facts.


---

## 96. [Using Decentralized Aggregation for Federated Learning with Differential Privacy](https://arxiv.org/abs/2311.16008) <a name="link96"></a>
**ArXiv ID:** 2311.16008
**Authors:** Hadeel Abd El-Kareem, Abd El-Moaty Saleh, Ana Fernández-Vilas, Manuel Fernández-Veiga, asser El-Sonbaty

**Abstract:** Nowadays, the ubiquitous usage of mobile devices and networks have raised
concerns about the loss of control over personal data and research advance
towards the trade-off between privacy and utility in scenarios that combine
exchange communications, big databases and distributed and collaborative (P2P)
Machine Learning techniques. On the other hand, although Federated Learning
(FL) provides some level of privacy by retaining the data at the local node,
which executes a local training to enrich a global model, this scenario is
still susceptible to privacy breaches as membership inference attacks. To
provide a stronger level of privacy, this research deploys an experimental
environment for FL with Differential Privacy (DP) using benchmark datasets. The
obtained results show that the election of parameters and techniques of DP is
central in the aforementioned trade-off between privacy and utility by means of
a classification example.


---

## 97. [Over-Squashing in Riemannian Graph Neural Networks](https://arxiv.org/abs/2311.15945) <a name="link97"></a>
**ArXiv ID:** 2311.15945
**Authors:** Julia Balla

**Abstract:** Most graph neural networks (GNNs) are prone to the phenomenon of
over-squashing in which node features become insensitive to information from
distant nodes in the graph. Recent works have shown that the topology of the
graph has the greatest impact on over-squashing, suggesting graph rewiring
approaches as a suitable solution. In this work, we explore whether
over-squashing can be mitigated through the embedding space of the GNN. In
particular, we consider the generalization of Hyperbolic GNNs (HGNNs) to
Riemannian manifolds of variable curvature in which the geometry of the
embedding space is faithful to the graph's topology. We derive bounds on the
sensitivity of the node features in these Riemannian GNNs as the number of
layers increases, which yield promising theoretical and empirical results for
alleviating over-squashing in graphs with negative curvature.


---

## 98. [Machine learning and Topological data analysis identify unique features of human papillae in 3D scans](https://arxiv.org/abs/2307.06255) <a name="link98"></a>
**ArXiv ID:** 2307.06255
**Authors:** Rayna Andreeva, Anwesha Sarkar, Rik Sarkar

**Abstract:** The tongue surface houses a range of papillae that are integral to the
mechanics and chemistry of taste and textural sensation. Although gustatory
function of papillae is well investigated, the uniqueness of papillae within
and across individuals remains elusive. Here, we present the first machine
learning framework on 3D microscopic scans of human papillae (n = 2092),
uncovering the uniqueness of geometric and topological features of papillae.
The finer differences in shapes of papillae are investigated computationally
based on a number of features derived from discrete differential geometry and
computational topology. Interpretable machine learning techniques show that
persistent homology features of the papillae shape are the most effective in
predicting the biological variables. Models trained on these features with
small volumes of data samples predict the type of papillae with an accuracy of
85%. The papillae type classification models can map the spatial arrangement of
filiform and fungiform papillae on a surface. Remarkably, the papillae are
found to be distinctive across individuals and an individual can be identified
with an accuracy of 48% among the 15 participants from a single papillae.
Collectively, this is the first unprecedented evidence demonstrating that
tongue papillae can serve as a unique identifier inspiring new research
direction for food preferences and oral diagnostics.


---

## 99. [Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift](https://arxiv.org/abs/2311.15961) <a name="link99"></a>
**ArXiv ID:** 2311.15961
**Authors:** Jiawei Ge, Shange Tang, Jianqing Fan, Cong Ma, Chi Jin

**Abstract:** A key challenge of modern machine learning systems is to achieve
Out-of-Distribution (OOD) generalization -- generalizing to target data whose
distribution differs from that of source data. Despite its significant
importance, the fundamental question of ``what are the most effective
algorithms for OOD generalization'' remains open even under the standard
setting of covariate shift. This paper addresses this fundamental question by
proving that, surprisingly, classical Maximum Likelihood Estimation (MLE)
purely using source data (without any modification) achieves the minimax
optimality for covariate shift under the well-specified setting. That is, no
algorithm performs better than MLE in this setting (up to a constant factor),
justifying MLE is all you need. Our result holds for a very rich class of
parametric models, and does not require any boundedness condition on the
density ratio. We illustrate the wide applicability of our framework by
instantiating it to three concrete examples -- linear regression, logistic
regression, and phase retrieval. This paper further complement the study by
proving that, under the misspecified setting, MLE is no longer the optimal
choice, whereas Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax
optimal in certain scenarios.


---

## 100. [Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale](https://arxiv.org/abs/2311.15816) <a name="link100"></a>
**ArXiv ID:** 2311.15816
**Authors:** Soyed Tuhin Ahmed, Kamal Danouchi, Michael Hefenbrock, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori

**Abstract:** Uncertainty estimation in Neural Networks (NNs) is vital in improving
reliability and confidence in predictions, particularly in safety-critical
applications. Bayesian Neural Networks (BayNNs) with Dropout as an
approximation offer a systematic approach to quantifying uncertainty, but they
inherently suffer from high hardware overhead in terms of power, memory, and
computation. Thus, the applicability of BayNNs to edge devices with limited
resources or to high-performance applications is challenging. Some of the
inherent costs of BayNNs can be reduced by accelerating them in hardware on a
Computation-In-Memory (CIM) architecture with spintronic memories and
binarizing their parameters. However, numerous stochastic units are required to
implement conventional dropout-based BayNN. In this paper, we propose the Scale
Dropout, a novel regularization technique for Binary Neural Networks (BNNs),
and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient
uncertainty estimation. Our approach requires only one stochastic unit for the
entire model, irrespective of the model size, leading to a highly scalable
Bayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM
architecture for the proposed BayNN that achieves more than $100\times$ energy
savings compared to the state-of-the-art. We validated our method to show up to
a $1\%$ improvement in predictive performance and superior uncertainty
estimates compared to related works.


---

## 101. [Low-degree learning and the metric entropy of polynomials](https://arxiv.org/abs/2203.09659) <a name="link101"></a>
**ArXiv ID:** 2203.09659
**Authors:** Alexandros Eskenazis, Paata Ivanisvili, Lauritz Streck

**Abstract:** Let $\mathscr{F}_{n,d}$ be the class of all functions $f:\{-1,1\}^n\to[-1,1]$
on the $n$-dimensional discrete hypercube of degree at most $d$. In the first
part of this paper, we prove that any (deterministic or randomized) algorithm
which learns $\mathscr{F}_{n,d}$ with $L_2$-accuracy $\varepsilon$ requires at
least $\Omega((1-\sqrt{\varepsilon})2^d\log n)$ queries for large enough $n$,
thus establishing the sharpness as $n\to\infty$ of a recent upper bound of
Eskenazis and Ivanisvili (2021). To do this, we show that the $L_2$-packing
numbers $\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon)$ of the
concept class $\mathscr{F}_{n,d}$ satisfy the two-sided estimate
$$c(1-\varepsilon)2^d\log n \leq \log
\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon) \leq \frac{2^{Cd}\log
n}{\varepsilon^4}$$ for large enough $n$, where $c, C>0$ are universal
constants. In the second part of the paper, we present a logarithmic upper
bound for the randomized query complexity of classes of bounded approximate
polynomials whose Fourier spectra are concentrated on few subsets. As an
application, we prove new estimates for the number of random queries required
to learn approximate juntas of a given degree, functions with rapidly decaying
Fourier tails and constant depth circuits of given size. Finally, we obtain
bounds for the number of queries required to learn the polynomial class
$\mathscr{F}_{n,d}$ without error in the query and random example models.


---

## 102. [WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models](https://arxiv.org/abs/2311.15930) <a name="link102"></a>
**ArXiv ID:** 2311.15930
**Authors:** Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, Pascal Vincent

**Abstract:** We propose WorldSense, a benchmark designed to assess the extent to which
LLMs are consistently able to sustain tacit world models, by testing how they
draw simple inferences from descriptions of simple arrangements of entities.
Worldsense is a synthetic benchmark with three problem types, each with their
own trivial control, which explicitly avoids bias by decorrelating the abstract
structure of problems from the vocabulary and expressions, and by decorrelating
all problem subparts with the correct response. We run our benchmark on three
state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these
models make errors even with as few as three objects. Furthermore, they have
quite heavy response biases, preferring certain responses irrespective of the
question. Errors persist even with chain-of-thought prompting and in-context
learning. Lastly, we show that while finetuning on similar problems does result
in substantial improvements -- within- and out-of-distribution -- the finetuned
models do not generalise beyond a constraint problem space.


---

## 103. [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037) <a name="link103"></a>
**ArXiv ID:** 2308.07037
**Authors:** Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez

**Abstract:** This paper introduces Bayesian Flow Networks (BFNs), a new class of
generative model in which the parameters of a set of independent distributions
are modified with Bayesian inference in the light of noisy data samples, then
passed as input to a neural network that outputs a second, interdependent
distribution. Starting from a simple prior and iteratively updating the two
distributions yields a generative procedure similar to the reverse process of
diffusion models; however it is conceptually simpler in that no forward process
is required. Discrete and continuous-time loss functions are derived for
continuous, discretised and discrete data, along with sample generation
procedures. Notably, the network inputs for discrete data lie on the
probability simplex, and are therefore natively differentiable, paving the way
for gradient-based sample guidance and few-step generation in discrete domains
such as language modelling. The loss function directly optimises data
compression and places no restrictions on the network architecture. In our
experiments BFNs achieve competitive log-likelihoods for image modelling on
dynamically binarized MNIST and CIFAR-10, and outperform all known discrete
diffusion models on the text8 character-level language modelling task.


---

## 104. [Reinforcement Learning for Wildfire Mitigation in Simulated Disaster Environments](https://arxiv.org/abs/2311.15925) <a name="link104"></a>
**ArXiv ID:** 2311.15925
**Authors:** Alexander Tapley, Marissa Dotter, Michael Doyle, Aidan Fennelly, Dhanuj Gandikota, Savanna Smith, Michael Threet, Tim Welsh

**Abstract:** Climate change has resulted in a year over year increase in adverse weather
and weather conditions which contribute to increasingly severe fire seasons.
Without effective mitigation, these fires pose a threat to life, property,
ecology, cultural heritage, and critical infrastructure. To better prepare for
and react to the increasing threat of wildfires, more accurate fire modelers
and mitigation responses are necessary. In this paper, we introduce SimFire, a
versatile wildland fire projection simulator designed to generate realistic
wildfire scenarios, and SimHarness, a modular agent-based machine learning
wrapper capable of automatically generating land management strategies within
SimFire to reduce the overall damage to the area. Together, this publicly
available system allows researchers and practitioners the ability to emulate
and assess the effectiveness of firefighter interventions and formulate
strategic plans that prioritize value preservation and resource allocation
optimization. The repositories are available for download at
https://github.com/mitrefireline.


---

## 105. [On the Effectiveness of Log Representation for Log-based Anomaly Detection](https://arxiv.org/abs/2308.08736) <a name="link105"></a>
**ArXiv ID:** 2308.08736
**Authors:** Xingfang Wu, Heng Li, Foutse Khomh

**Abstract:** Logs are an essential source of information for people to understand the
running status of a software system. Due to the evolving modern software
architecture and maintenance methods, more research efforts have been devoted
to automated log analysis. In particular, machine learning (ML) has been widely
used in log analysis tasks. In ML-based log analysis tasks, converting textual
log data into numerical feature vectors is a critical and indispensable step.
However, the impact of using different log representation techniques on the
performance of the downstream models is not clear, which limits researchers and
practitioners' opportunities of choosing the optimal log representation
techniques in their automated log analysis workflows. Therefore, this work
investigates and compares the commonly adopted log representation techniques
from previous log analysis research. Particularly, we select six log
representation techniques and evaluate them with seven ML models and four
public log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of
log-based anomaly detection. We also examine the impacts of the log parsing
process and the different feature aggregation approaches when they are employed
with log representation techniques. From the experiments, we provide some
heuristic guidelines for future researchers and developers to follow when
designing an automated log analysis workflow. We believe our comprehensive
comparison of log representation techniques can help researchers and
practitioners better understand the characteristics of different log
representation techniques and provide them with guidance for selecting the most
suitable ones for their ML-based log analysis workflow.


---

## 106. [BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights](https://arxiv.org/abs/2311.16075) <a name="link106"></a>
**ArXiv ID:** 2311.16075
**Authors:** François Remy, Kris Demuynck, Thomas Demeester

**Abstract:** In this study, we investigate the potential of Large Language Models to
complement biomedical knowledge graphs in the training of semantic models for
the biomedical and clinical domains. Drawing on the wealth of the UMLS
knowledge graph and harnessing cutting-edge Large Language Models, we propose a
new state-of-the-art approach for obtaining high-fidelity representations of
biomedical concepts and sentences, consisting of three steps: an improved
contrastive learning phase, a novel self-distillation phase, and a weight
averaging phase. Through rigorous evaluations via the extensive BioLORD testing
suite and diverse downstream tasks, we demonstrate consistent and substantial
performance improvements over the previous state of the art (e.g. +2pts on
MedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new
state-of-the-art biomedical model for English, we also distill and release a
multilingual model compatible with 50+ languages and finetuned on 7 European
languages. Many clinical pipelines can benefit from our latest models. Our new
multilingual model enables a range of languages to benefit from our
advancements in biomedical semantic representation learning, opening a new
avenue for bioinformatics researchers around the world. As a result, we hope to
see BioLORD-2023 becoming a precious tool for future biomedical applications.


---

## 107. [FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations](https://arxiv.org/abs/2211.14309) <a name="link107"></a>
**ArXiv ID:** 2211.14309
**Authors:** Christian Diller, Thomas Funkhouser, Angela Dai

**Abstract:** We present a generative approach to forecast long-term future human behavior
in 3D, requiring only weak supervision from readily available 2D human action
data. This is a fundamental task enabling many downstream applications. The
required ground-truth data is hard to capture in 3D (mocap suits, expensive
setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our
method to only require 2D RGB data while being able to generate 3D human motion
sequences. We use a differentiable 2D projection scheme in an autoregressive
manner for weak supervision, and an adversarial loss for 3D regularization. Our
method predicts long and complex behavior sequences (e.g. cooking, assembly)
consisting of multiple sub-actions. We tackle this in a semantically
hierarchical manner, jointly predicting high-level coarse action labels
together with their low-level fine-grained realizations as characteristic 3D
human poses. We observe that these two action representations are coupled in
nature, and joint prediction benefits both action and pose forecasting. Our
experiments demonstrate the complementary nature of joint action and 3D pose
prediction: our joint approach outperforms each task treated individually,
enables robust longer-term sequence prediction, and outperforms alternative
approaches to forecast actions and characteristic 3D poses.


---

## 108. [Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models](https://arxiv.org/abs/2311.16103) <a name="link108"></a>
**ArXiv ID:** 2311.16103
**Authors:** Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan

**Abstract:** Video-based large language models (Video-LLMs) have been recently introduced,
targeting both fundamental improvements in perception and comprehension, and a
diverse range of user inquiries. In pursuit of the ultimate goal of achieving
artificial general intelligence, a truly intelligent Video-LLM model should not
only see and understand the surroundings, but also possess human-level
commonsense, and make well-informed decisions for the users. To guide the
development of such a model, the establishment of a robust and comprehensive
evaluation system becomes crucial. To this end, this paper proposes
\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit
specifically designed for evaluating Video-LLMs. The benchmark comprises 10
meticulously crafted tasks, evaluating the capabilities of Video-LLMs across
three distinct levels: Video-exclusive Understanding, Prior Knowledge-based
Question-Answering, and Comprehension and Decision-making. In addition, we
introduce an automatic toolkit tailored to process model outputs for various
tasks, facilitating the calculation of metrics and generating convenient final
scores. We evaluate 8 representative Video-LLMs using \textit{Video-Bench}. The
findings reveal that current Video-LLMs still fall considerably short of
achieving human-like comprehension and analysis of real-world videos, offering
valuable insights for future research directions. The benchmark and toolkit are
available at: \url{https://github.com/PKU-YuanGroup/Video-Bench}.


---

## 109. [Automated Measurement of Vascular Calcification in Femoral Endarterectomy Patients Using Deep Learning](https://arxiv.org/abs/2311.16001) <a name="link109"></a>
**ArXiv ID:** 2311.16001
**Authors:** Alireza Bagheri Rajeoni, Breanna Pederson, Daniel G. Clair, Susan M. Lessner, Homayoun Valafar

**Abstract:** Atherosclerosis, a chronic inflammatory disease affecting the large arteries,
presents a global health risk. Accurate analysis of diagnostic images, like
computed tomographic angiograms (CTAs), is essential for staging and monitoring
the progression of atherosclerosis-related conditions, including peripheral
arterial disease (PAD). However, manual analysis of CTA images is
time-consuming and tedious. To address this limitation, we employed a deep
learning model to segment the vascular system in CTA images of PAD patients
undergoing femoral endarterectomy surgery and to measure vascular calcification
from the left renal artery to the patella. Utilizing proprietary CTA images of
27 patients undergoing femoral endarterectomy surgery provided by Prisma Health
Midlands, we developed a Deep Neural Network (DNN) model to first segment the
arterial system, starting from the descending aorta to the patella, and second,
to provide a metric of arterial calcification. Our designed DNN achieved 83.4%
average Dice accuracy in segmenting arteries from aorta to patella, advancing
the state-of-the-art by 0.8%. Furthermore, our work is the first to present a
robust statistical analysis of automated calcification measurement in the lower
extremities using deep learning, attaining a Mean Absolute Percentage Error
(MAPE) of 9.5% and a correlation coefficient of 0.978 between automated and
manual calcification scores. These findings underscore the potential of deep
learning techniques as a rapid and accurate tool for medical professionals to
assess calcification in the abdominal aorta and its branches above the patella.
The developed DNN model and related documentation in this project are available
at GitHub page at https://github.com/pip-alireza/DeepCalcScoring.


---

## 110. [Car-Following Models: A Multidisciplinary Review](https://arxiv.org/abs/2304.07143) <a name="link110"></a>
**ArXiv ID:** 2304.07143
**Authors:** Tianya Zhang, Peter J. Jin, Alexandre Bayen, Ph. D., Benedetto Piccoli

**Abstract:** Car-following (CF) algorithms are crucial components of traffic simulations
and have been integrated into many production vehicles equipped with Advanced
Driving Assistance Systems (ADAS). Insights from the model of car-following
behavior help us understand the causes of various macro phenomena that arise
from interactions between pairs of vehicles. Car-following models encompass
multiple disciplines, including traffic engineering, physics, dynamic system
control, cognitive science, machine learning, and reinforcement learning. This
paper presents an extensive survey that highlights the differences,
complementarities, and overlaps among microscopic traffic flow and control
models based on their underlying principles and design logic. It reviews
representative algorithms, ranging from theory-based kinematic models,
Psycho-Physical Models, and Adaptive cruise control models to data-driven
algorithms like Reinforcement Learning and Imitation Learning (IL). The
manuscript discusses the strengths and limitations of these models and explores
their applications in different contexts. This review synthesizes existing
researches across different domains to fill knowledge gaps and offer guidance
for future research by identifying the latest trends in car following models
and their applications.


---

## 111. [ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting](https://arxiv.org/abs/2310.13258) <a name="link111"></a>
**ArXiv ID:** 2310.13258
**Authors:** Kushal Kedia, Prithwish Dan, Atiksh Bhardwaj, Sanjiban Choudhury

**Abstract:** Seamless human-robot manipulation in close proximity relies on accurate
forecasts of human motion. While there has been significant progress in
learning forecast models at scale, when applied to manipulation tasks, these
models accrue high errors at critical transition points leading to degradation
in downstream planning performance. Our key insight is that instead of
predicting the most likely human motion, it is sufficient to produce forecasts
that capture how future human motion would affect the cost of a robot's plan.
We present ManiCast, a novel framework that learns cost-aware human forecasts
and feeds them to a model predictive control planner to execute collaborative
manipulation tasks. Our framework enables fluid, real-time interactions between
a human and a 7-DoF robot arm across a number of real-world tasks such as
reactive stirring, object handovers, and collaborative table setting. We
evaluate both the motion forecasts and the end-to-end forecaster-planner system
against a range of learned and heuristic baselines while additionally
contributing new datasets. We release our code and datasets at
https://portal-cornell.github.io/manicast/.


---

## 112. [Machine Learning-Enhanced Aircraft Landing Scheduling under Uncertainties](https://arxiv.org/abs/2311.16030) <a name="link112"></a>
**ArXiv ID:** 2311.16030
**Authors:** Yutian Pang, Peng Zhao, Jueming Hu, Yongming Liu

**Abstract:** This paper addresses aircraft delays, emphasizing their impact on safety and
financial losses. To mitigate these issues, an innovative machine learning
(ML)-enhanced landing scheduling methodology is proposed, aiming to improve
automation and safety. Analyzing flight arrival delay scenarios reveals strong
multimodal distributions and clusters in arrival flight time durations. A
multi-stage conditional ML predictor enhances separation time prediction based
on flight events. ML predictions are then integrated as safety constraints in a
time-constrained traveling salesman problem formulation, solved using
mixed-integer linear programming (MILP). Historical flight recordings and model
predictions address uncertainties between successive flights, ensuring
reliability. The proposed method is validated using real-world data from the
Atlanta Air Route Traffic Control Center (ARTCC ZTL). Case studies demonstrate
an average 17.2% reduction in total landing time compared to the
First-Come-First-Served (FCFS) rule. Unlike FCFS, the proposed methodology
considers uncertainties, instilling confidence in scheduling. The study
concludes with remarks and outlines future research directions.


---

## 113. [Cell Maps Representation For Lung Adenocarcinoma Growth Patterns Classification In Whole Slide Images](https://arxiv.org/abs/2311.15847) <a name="link113"></a>
**ArXiv ID:** 2311.15847
**Authors:** Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, Nasir Rajpoot, Shan E Ahmed Raza

**Abstract:** Lung adenocarcinoma is a morphologically heterogeneous disease, characterized
by five primary histologic growth patterns. The quantity of these patterns can
be related to tumor behavior and has a significant impact on patient prognosis.
In this work, we propose a novel machine learning pipeline capable of
classifying tissue tiles into one of the five patterns or as non-tumor, with an
Area Under the Receiver Operating Characteristic Curve (AUCROC) score of 0.97.
Our model's strength lies in its comprehensive consideration of cellular
spatial patterns, where it first generates cell maps from Hematoxylin and Eosin
(H&E) whole slide images (WSIs), which are then fed into a convolutional neural
network classification model. Exploiting these cell maps provides the model
with robust generalizability to new data, achieving approximately 30% higher
accuracy on unseen test-sets compared to current state of the art approaches.
The insights derived from our model can be used to predict prognosis, enhancing
patient outcomes.


---

## 114. [A Quantitative Approach to Understand Self-Supervised Models as Cross-lingual Feature Extractors](https://arxiv.org/abs/2311.15954) <a name="link114"></a>
**ArXiv ID:** 2311.15954
**Authors:** Shuyue Stella Li, Beining Xu, Xiangyu Zhang, Hexin Liu, Wenhan Chao, Leibny Paola Garcia

**Abstract:** In this work, we study the features extracted by English self-supervised
learning (SSL) models in cross-lingual contexts and propose a new metric to
predict the quality of feature representations. Using automatic speech
recognition (ASR) as a downstream task, we analyze the effect of model size,
training objectives, and model architecture on the models' performance as a
feature extractor for a set of topologically diverse corpora. We develop a
novel metric, the Phonetic-Syntax Ratio (PSR), to measure the phonetic and
synthetic information in the extracted representations using deep generalized
canonical correlation analysis. Results show the contrastive loss in the
wav2vec2.0 objective facilitates more effective cross-lingual feature
extraction. There is a positive correlation between PSR scores and ASR
performance, suggesting that phonetic information extracted by monolingual SSL
models can be used for downstream tasks in cross-lingual settings. The proposed
metric is an effective indicator of the quality of the representations and can
be useful for model selection.


---

## 115. [Test-time Adaptation of Discriminative Models via Diffusion Generative Feedback](https://arxiv.org/abs/2311.16102) <a name="link115"></a>
**ArXiv ID:** 2311.16102
**Authors:** Mihir Prabhudesai, Tsung-Wei Ke, Alexander C. Li, Deepak Pathak, Katerina Fragkiadaki

**Abstract:** The advancements in generative modeling, particularly the advent of diffusion
models, have sparked a fundamental question: how can these models be
effectively used for discriminative tasks? In this work, we find that
generative models can be great test-time adapters for discriminative models.
Our method, Diffusion-TTA, adapts pre-trained discriminative models such as
image classifiers, segmenters and depth predictors, to each unlabelled example
in the test set using generative feedback from a diffusion model. We achieve
this by modulating the conditioning of the diffusion model using the output of
the discriminative model. We then maximize the image likelihood objective by
backpropagating the gradients to discriminative model's parameters. We show
Diffusion-TTA significantly enhances the accuracy of various large-scale
pre-trained discriminative models, such as, ImageNet classifiers, CLIP models,
image pixel labellers and image depth predictors. Diffusion-TTA outperforms
existing test-time adaptation methods, including TTT-MAE and TENT, and
particularly shines in online adaptation setups, where the discriminative model
is continually adapted to each example in the test set. We provide access to
code, results, and visualizations on our website:
https://diffusion-tta.github.io/.


---

## 116. [CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](https://arxiv.org/abs/2306.00349) <a name="link116"></a>
**ArXiv ID:** 2306.00349
**Authors:** Jiachen Sun, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Z. Morley Mao, Chaowei Xiao

**Abstract:** Perception is crucial in the realm of autonomous driving systems, where
bird's eye view (BEV)-based architectures have recently reached
state-of-the-art performance. The desirability of self-supervised
representation learning stems from the expensive and laborious process of
annotating 2D and 3D data. Although previous research has investigated
pretraining methods for both LiDAR and camera-based 3D object detection, a
unified pretraining framework for multimodal BEV perception is missing. In this
study, we introduce CALICO, a novel framework that applies contrastive
objectives to both LiDAR and camera backbones. Specifically, CALICO
incorporates two stages: point-region contrast (PRC) and region-aware
distillation (RAD). PRC better balances the region- and scene-level
representation learning on the LiDAR modality and offers significant
performance improvement compared to existing methods. RAD effectively achieves
contrastive distillation on our self-trained teacher model. CALICO's efficacy
is substantiated by extensive evaluations on 3D object detection and BEV map
segmentation tasks, where it delivers significant performance improvements.
Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS and
mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection
against adversarial attacks and corruption. Additionally, our framework can be
tailored to different backbones and heads, positioning it as a promising
approach for multimodal BEV perception.


---

## 117. [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks](https://arxiv.org/abs/2305.13547) <a name="link117"></a>
**ArXiv ID:** 2305.13547
**Authors:** Haoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Dongsheng Li, Dacheng Tao

**Abstract:** Text classification tasks often encounter few shot scenarios with limited
labeled data, and addressing data scarcity is crucial. Data augmentation with
mixup has shown to be effective on various text classification tasks. However,
most of the mixup methods do not consider the varying degree of learning
difficulty in different stages of training and generate new samples with one
hot labels, resulting in the model over confidence. In this paper, we propose a
self evolution learning (SE) based mixup approach for data augmentation in text
classification, which can generate more adaptive and model friendly pesudo
samples for the model training. SE focuses on the variation of the model's
learning ability. To alleviate the model confidence, we introduce a novel
instance specific label smoothing approach, which linearly interpolates the
model's output and one hot labels of the original samples to generate new soft
for label mixing up. Through experimental analysis, in addition to improving
classification accuracy, we demonstrate that SE also enhances the model's
generalize ability.


---

## 118. [AST: Effective Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories](https://arxiv.org/abs/2310.10541) <a name="link118"></a>
**ArXiv ID:** 2310.10541
**Authors:** Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam

**Abstract:** Training large AI models typically requires large-scale datasets in the
machine learning process, making training and parameter-tuning process both
time-consuming and costly. Some researchers address this problem by carefully
synthesizing a very small number of highly representative and informative
samples from real-world datasets. This approach, known as Dataset Distillation
(DD), proposes a perspective for data-efficient learning. Despite recent
progress in this field, the performance of existing methods still cannot meet
expectations, and distilled datasets cannot effectively replace original
datasets. In this paper, unlike previous methods that focus solely on improving
the effectiveness of student distillation, we recognize and leverage the
important mutual influence between expert and student models. We observed that
the smoothness of expert trajectories has a significant impact on subsequent
student parameter alignment. Based on this, we propose an effective DD
framework named AST, standing for Alignment with Smooth and high-quality expert
Trajectories. We devise the integration of clipping loss and gradient penalty
to regulate the rate of parameter changes in expert trajectory generation. To
further refine the student parameter alignment with expert trajectory, we put
forward representative initialization for the synthetic dataset and balanced
inner-loop loss in response to the sensitivity exhibited towards randomly
initialized variables during distillation. We also propose two enhancement
strategies, namely intermediate matching loss and weight perturbation, to
mitigate the potential occurrence of cumulative errors. We conduct extensive
experiments on datasets of different scales, sizes, and resolutions. The
results demonstrate that the proposed method significantly outperforms prior
methods.


---

## 119. [Efficient Pre-training for Localized Instruction Generation of Videos](https://arxiv.org/abs/2311.15964) <a name="link119"></a>
**ArXiv ID:** 2311.15964
**Authors:** Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller

**Abstract:** Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.


---

## 120. [A Fully Data-Driven Approach for Realistic Traffic Signal Control Using Offline Reinforcement Learning](https://arxiv.org/abs/2311.15920) <a name="link120"></a>
**ArXiv ID:** 2311.15920
**Authors:** Jianxiong Li, Shichao Lin, Tianyu Shi, Chujie Tian, Yu Mei, Jian Song, Xianyuan Zhan, Ruimin Li

**Abstract:** The optimization of traffic signal control (TSC) is critical for an efficient
transportation system. In recent years, reinforcement learning (RL) techniques
have emerged as a popular approach for TSC and show promising results for
highly adaptive control. However, existing RL-based methods suffer from notably
poor real-world applicability and hardly have any successful deployments. The
reasons for such failures are mostly due to the reliance on over-idealized
traffic simulators for policy optimization, as well as using unrealistic
fine-grained state observations and reward signals that are not directly
obtainable from real-world sensors. In this paper, we propose a fully
Data-Driven and simulator-free framework for realistic Traffic Signal Control
(D2TSC). Specifically, we combine well-established traffic flow theory with
machine learning to construct a reward inference model to infer the reward
signals from coarse-grained traffic data. With the inferred rewards, we further
propose a sample-efficient offline RL method to enable direct signal control
policy learning from historical offline datasets of real-world intersections.
To evaluate our approach, we collect historical traffic data from a real-world
intersection, and develop a highly customized simulation environment that
strictly follows real data characteristics. We demonstrate through extensive
experiments that our approach achieves superior performance over conventional
and offline RL baselines, and also enjoys much better real-world applicability.


---

## 121. [Replay across Experiments: A Natural Extension of Off-Policy RL](https://arxiv.org/abs/2311.15951) <a name="link121"></a>
**ArXiv ID:** 2311.15951
**Authors:** Dhruva Tirumala, Thomas Lampe, Jose Enrique Chen, Tuomas Haarnoja, Sandy Huang, Guy Lever, Ben Moran, Tim Hertweck, Leonard Hasenclever, Martin Riedmiller, Nicolas Heess, Markus Wulfmeier

**Abstract:** Replaying data is a principal mechanism underlying the stability and data
efficiency of off-policy reinforcement learning (RL). We present an effective
yet simple framework to extend the use of replays across multiple experiments,
minimally adapting the RL workflow for sizeable improvements in controller
performance and research iteration times. At its core, Replay Across
Experiments (RaE) involves reusing experience from previous experiments to
improve exploration and bootstrap learning while reducing required changes to a
minimum in comparison to prior work. We empirically show benefits across a
number of RL algorithms and challenging control domains spanning both
locomotion and manipulation, including hard exploration tasks from egocentric
vision. Through comprehensive ablations, we demonstrate robustness to the
quality and amount of data available and various hyperparameter choices.
Finally, we discuss how our approach can be applied more broadly across
research life cycles and can increase resilience by reloading data across
random seeds or hyperparameter variations.


---

## 122. [Physics-informed neural networks for transformed geometries and manifolds](https://arxiv.org/abs/2311.15940) <a name="link122"></a>
**ArXiv ID:** 2311.15940
**Authors:** Samuel Burbulla

**Abstract:** Physics-informed neural networks (PINNs) effectively embed physical
principles into machine learning, but often struggle with complex or
alternating geometries. We propose a novel method for integrating geometric
transformations within PINNs to robustly accommodate geometric variations. Our
method incorporates a diffeomorphism as a mapping of a reference domain and
adapts the derivative computation of the physics-informed loss function. This
generalizes the applicability of PINNs not only to smoothly deformed domains,
but also to lower-dimensional manifolds and allows for direct shape
optimization while training the network. We demonstrate the effectivity of our
approach on several problems: (i) Eikonal equation on Archimedean spiral, (ii)
Poisson problem on surface manifold, (iii) Incompressible Stokes flow in
deformed tube, and (iv) Shape optimization with Laplace operator. Through these
examples, we demonstrate the enhanced flexibility over traditional PINNs,
especially under geometric variations. The proposed framework presents an
outlook for training deep neural operators over parametrized geometries, paving
the way for advanced modeling with PDEs on complex geometries in science and
engineering.


---
